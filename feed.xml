<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://riyadhrazzaq.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://riyadhrazzaq.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-29T11:06:59+00:00</updated><id>https://riyadhrazzaq.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Paper Notes / Contrastive Predicting Coding</title><link href="https://riyadhrazzaq.github.io/blog/2024/contrastive-predictive-coding/" rel="alternate" type="text/html" title="Paper Notes / Contrastive Predicting Coding"/><published>2024-11-15T00:01:13+00:00</published><updated>2024-11-15T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/contrastive-predictive-coding</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/contrastive-predictive-coding/"><![CDATA[<p>This is an attempt to write down my understanding of the paper “<a href="http://arxiv.org/abs/1807.03748">Representation Learning with Contrastive Predictive Coding” by Oord et al.</a></p> <h1 id="ultimate-goal">Ultimate Goal</h1> <p>The primary goal is to learn speech representation that can be further used on numerous downstream tasks such as ASR, ST etc. Now, language models typically learn this sort of representation through next token prediction. The model produces a latent representation from which the output distribution for the next token is generated. The loss function is usually a cross-entropy function.</p> <p>This approach is not suitable for speech because the speech signal is high dimensional whether it is raw wave signals or log mel-spectrograms. And high dimensional signals do not produce strong results for such a task.</p> <h1 id="the-re-oriented-approach">The Re-oriented Approach</h1> <p>Assume we have continuous signals \(\vec{X} = \\{\vec{x_1},...,\vec{x_M}\\}\). Each \(x_i\) is a vector of signal representation. Given \(x_t\), we will now predict the latent representation for \(x_{t+k}\), where \(k &gt; t\). This is presented in the paper in the following way: instead of modelling \(p(\mathbf{x_{t+k} | c_t})\), this approach will model \(f_k(\mathbf{x_{t+k}, c_t}) = exp(\mathbf{z_{t+k}^T \cdot W_k \cdot c_t})\) which is a density ratio between the context at \(t^{th}\) step to the latent representation at \(t+k^{th}\) step. Here, \(c_t\) is the context representation and \(z_t\) is the latent representation. In some way, we are looking for the relationship between the two and our loss function will reflect that more clearly.</p> <h1 id="training">Training</h1> <h2 id="forward">Forward</h2> <p>We have an encoder layer that will generate latent representation \(\mathbf{z_t}\) and another auto-regressive layer which will generate \(\mathbf{c_t}\) from \(\mathbf{z_t}\) and \(\mathbf{c_{p &lt; t}}\).</p> <h2 id="backward">backward</h2> <p>Instead of directly predicting the latent representation, the model will learn to contrast between the latent representations. We want the predicted latent representation for the current time step to be similar to the \(k^{th}\) tokens after it and different from other representations. We call them positive and negative samples respectively.</p> <p>The model is given \(N\) samples among which \(N-1\) are negative samples. The positive sample is drawn from a regular conditional model \(p(\mathbf{x_{t+k}|c_t})\) (this can be an n-gram model). The negative samples are drawn from \(p(\mathbf{x_{t+k}})\) - which is an overall collection of all the possible values (like the vocab in a text LLM). \(\begin{align*} L_N &amp;= - \Bbb{E} log \frac{f_k(\mathbf{x_{t+k}, c_t})}{\sum_{x_j \in \mathbf{X}} f_k(\mathbf{x_j, c_t}) } \\ &amp;= - \Bbb{E} log \frac{exp(\mathbf{z_{t+k}^T W_k c_t})}{\sum_{x_j \in \mathbf{X}} exp(\mathbf{z_j^T W_k c_t})} \\ &amp;= - \Bbb{E} log \frac{cosine\ similarity\ between\ current\ context\ repr.\ and\ future\ true\ latent\ repr.}{sum\ of\ the\ cosine\ similarities\ between\ all\ the\ pos \&amp; neg\ future\ samples} \end{align*}\)</p> <p>This loss penalizes the model if the cosine similarity between true latent representation is larger compared to all the cosine similarity it calculated The only way for the model to perform well is to learn to contrast between the positive sample and the other \(N-1\) negative samples.</p> <h2 id="some-comments">Some Comments</h2> <p>In the loss function, \(W_k\) is different for each \(k\). So, we can swap the weights afterwards if we want the model to perform the prediction for the next step or \(k\) steps after that. Even though we are sampling \(N\) samples, whether be it positive or negative, the loss is calculated from the latent representation of that \(\mathbf{x}\)</p> <h1 id="references">References</h1> <ol> <li>Oord, A. van den, Li, Y., &amp; Vinyals, O. (2019). Representation Learning with Contrastive Predictive Coding (arXiv:1807.03748). arXiv. http://arxiv.org/abs/1807.03748</li> <li>E. Hinton, G. (2013). LEARNING DISTRIBUTED REPRESENTATIONS FOR STATISTICAL LANGUAGE MODELLING. Retrieved November 16, 2024, from http://www.cs.utoronto.ca/%7Ehinton/csc2535/notes/hlbl.pdf</li> </ol>]]></content><author><name></name></author><category term="machine-learning"/><category term="paper"/><category term="speech"/><summary type="html"><![CDATA[my notes on the paper 'Representation Learning with Contrastive Predictive Coding' by Oord et al.]]></summary></entry><entry><title type="html">আমার ইরাস্মুসে আবেদন</title><link href="https://riyadhrazzaq.github.io/blog/2024/lct/" rel="alternate" type="text/html" title="আমার ইরাস্মুসে আবেদন"/><published>2024-08-23T00:01:13+00:00</published><updated>2024-08-23T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/lct</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/lct/"><![CDATA[<p>২০২২ এ GRE ও IELTS পরীক্ষা দেই। পরেরটায় আলহামদুলিল্লাহ ভালো ফলাফল আসলেও প্রথমটায় একদমই খারাপ করি। নিচে আমার শিক্ষা জীবনের ফলাফলের সারাংশ দেয়া হলঃ</p> <blockquote> <p>এস এস সিঃ ৫.০০ <br/> এইচ এস সিঃ ৫.০০ <br/> ব্যাচেলরঃ ৩.৭০ ইউনিভার্সিটি অফ এশিয়া প্যাসিফিক থেকে <br/> IELTS: 7.5 (L9, R8, W7, S6.5)</p> </blockquote> <p>GRE তে খারাপ করার পর মানসিকভাবে ভেঙে পরি। যদিও আমার তখনকার উদ্দ্যেশ্য ছিল আমেরিকায় পি এইচ ডি প্রোগ্রামে আবেদন করা, কিন্তু ঐ পরিস্থিতিতে আমি আত্মবিশ্বাস রাখতে পারি নাই। কিছুদিন হাহুতাশ করার পর সিদ্ধান্ত নেই ইরাস্মুস প্রোগ্রামে আবেদন করবো। নভেম্বরের শেষে এই সিদ্ধান্ত নেয়ার পেছনে শুধু একটাই কারন ছিল, আমি পরের বছর আমেরিকায় আবেদন করার আগে একবার SOP লেখা এবং উচ্চ শিক্ষায় আবেদনের পুরো প্রক্রিয়ার একটা অভিজ্ঞতা নিতে চাচ্ছিলাম।</p> <p>প্রথম ধাপেই আমি ইরাস্মুস প্রোগ্রামের তালিকা থেকে নিজের পছন্দসই ৫ টা নির্বাচন করি। ইরাস্মুসে সীমাহীন প্রোগ্রামে আবেদন করা গেলেও, বাস্তবে তা করা যায় না। কারন সবগুলো প্রোগ্রামই আলাদা বৈচিত্র্যের, তাই আলাদা SOP লিখতে হবে। প্রোগ্রাম নির্বাচনের ক্ষেত্রে আমি নিম্নোক্ত বিষয়গুলোতে গুরুত্ব দেইঃ</p> <p>১। সব মাস্টার্স প্রোগ্রামই কোন নির্দিষ্ট বিষয়ে গুরুত্ব দিয়ে থাকে, যেমনঃ AI নিয়ে প্রোগ্রাম না করে, NLP কিংবা Medical AI নিয়ে প্রোগ্রাম বানায়। নিজের আগ্রহ এবং পূর্ববর্তী গবেষণার সাথে মিল থাকে এমন প্রোগ্রাম নির্বাচন করা উত্তম। <br/> ২। ইন্টার্নশিপ আছে নাকি নাই সেটাও খেয়াল করি। আমি ইন্টার্নশিপওয়ালা প্রোগ্রামে গুরুত্ব দেই বেশি, কারন এতে পরে চাকরি পেতে সুবিধা হবে বলে ধারনা করি। <br/> ৩। সাধারণত প্রোগ্রামের ওয়েবসাইটে এরা IELTS অথবা GRE নিয়ে কিছু বলে না। যদি বলে তবে আমি ঐ প্রোগ্রাম নির্বাচন করি, কারন IELTS এ ভালো করায় আমি ধরে নেই আমি অন্যান্য আবেদনকারীদের থেকে সামান্য হলেও এগিয়ে থাকবো (যদিও কতটুকু সত্য সেটা আমি জানি না, পুরোটাই আন্দাজ)।</p> <p>এইসকল বিষয় বিবেচনায় আমি EDISS, LCT, SECCLO, GENIAL, MAIA প্রোগ্রামগুলোর জন্য প্রস্তুতি নেই। প্রস্তুতি হিসেবে শুরুতেই কি কি কাগজপত্র লাগবে, SOP এর জন্য কিছু প্রোগ্রাম আলাদা নির্দেশনা দেয়, সেগুলো টুকে নেই। Recommendation Letter আরেকটি গুরুত্বপূর্ণ কিন্তু বিরক্তিকর বিষয়। আমার নির্বাচিত সব প্রোগ্রামেই ২ জনের সুপারিশ পত্র লেগেছিল। তাদেরকে আবেদনের সিদ্ধান্ত নেয়ার সাথে সাথেই অনুরোধ করে রাখি।</p> <p>আমি শুরুতে একটি SOP লিখি শুধু EDISS প্রোগ্রামের জন্য। দেড় পৃষ্ঠার ঐ লেখায় প্রথম অনুচ্ছেদে নিজের কম্পিউটার বিজ্ঞানে আগ্রহ কিভাবে আসে তা বলি। তারপরের অনুচ্ছেদে ব্যাচেলরে নিজের পড়াশোনা ও ফলাফল ছোট করে বিবৃতি দেই, তারপর জানাই কিভাবে NLP তে আগ্রহ এলো। আরেকটা অনুচ্ছেদে লিখি চাকুরীজীবনে কিসব দায়িত্ব নিয়ে কাজ করেছি। সর্বশেষ অনুচ্ছেদে এই প্রোগ্রামের সাথে নিজের আগ্রহের সম্পর্ক এবং এই প্রোগ্রাম যে আমাকে সাহায্য করবে কর্মজীবনের উদ্দেশ্য সাধনে তা বলি।</p> <p>তারপর এই SOP আমার বউ (উনিও বিদেশে উচ্চ শিক্ষায় গিয়েছেন) এবং আমার এক শিক্ষককে দেখাই। তাদের অনেক উপদেশের পর একটি গ্রহণযোগ্য খসড়া প্রস্তুত হয় এবং আমি সেটা অন্য প্রোগ্রামগুলোর জন্য পরিবর্তন করে নেই। এভাবে একমাস লাগিয়ে আমার SOP লেখা শেষ হয়। SOP একবার লিখেই শেষ করে দেয়া যায় না, যতবারই পড়ি ততবারই কিছু না কিছু হালকা পরিবর্তনের কথা মাথায় আসে। তাই SOP বেশ আগে থেকেই লেখা শুরু করা উচিত।</p> <p>সুপারিশপত্র বেশিরভাগ ক্ষেত্রেই শিক্ষক লিখে দেন। যদি নিজের লিখতে হয় সেক্ষেত্রে যতটা ব্যক্তিগত খাতিরের কথা লেখা যায় তত ভালো। খাতির মানে এই না যে উনি আপনাকে পছন্দ করতো সেটা লিখবেন, বরং শ্রেণীকক্ষে আপনার কি ধরনের কাজের জন্য আপনাকে উনি আপনাকে সুপারিশপত্র দিতে রাজি হয়েছে তা লিখবেন। আপনি শ্রেণীকক্ষে মনযোগী ছিলেন, কোন বিশেষ ল্যাব কিংবা এসাইনমেন্টে আপনার কাজ খুবই ভালো হয়েছিল এগুলা বলতে পারেন।</p> <p>বেশিরভাগ প্রোগ্রামেই আপনার CV চাইবে, সেক্ষেত্রে কর্মজীবনের আর শিক্ষাজীবনের CV আলাদা হয় সেটা লক্ষ্য রাখবেন। CV তে যায়গা সঙ্কট হলে প্রজেক্ট/পেপারের এর বিবরণ সংক্ষেপিত করা যায়। আবেদনের শেষ সীমার বেশ আগেই আবেদন করার চেষ্টা করবেন যাতে সুপারিশদাতারা যথেষ্ট সময় পান।</p> <p>২ মাস পর ফলাফল দিলে আমাকে দুই প্রোগ্রামে অপেক্ষমাণ তালিকায় রাখে। পরিশেষে LCT প্রোগ্রাম থেকে ডাক দেয়।</p>]]></content><author><name></name></author><category term="academia"/><category term="scholarship"/><summary type="html"><![CDATA[how I applied for the Erasmus Mundus scholarship.]]></summary></entry><entry><title type="html">Permutation and Combination</title><link href="https://riyadhrazzaq.github.io/blog/2024/counting/" rel="alternate" type="text/html" title="Permutation and Combination"/><published>2024-01-04T00:01:13+00:00</published><updated>2024-01-04T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/counting</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/counting/"><![CDATA[<p>Most of us are already introduced to permutation and combination in high school math classes. I understand clearly what they separately define. But I get confused easily between their equations because they never made sense to me. This post is my attempt to make sense of the following equations.<br/> \(\begin{align*} P_{n,r} &amp;= \frac{n!}{(n-r)!} \\ C_{n,r} &amp;= \frac{n!}{(n-r)!\ r!} \end{align*}\)</p> <h1 id="permutation">Permutation</h1> <p>I will attempt to figure out the number of ways to select 2 characters out of 3. The characters are {ABC}, \(n=3, r=2\). How many ways can I select 2 out of 3 characters? The answer is given by the permutation equation. Let me break it down to <em>options</em>. During the first selection, I have 3 options {ABC}, which will leave me 2 options for the second and for the final selection only 1 option will be left. As I get 3 options for the first, and for each of those first options, I get 2 sub-options, I have a total of \(3 \times 2 = 6\) options. However, it is not close to the \(P_{n,r}\) equation, but we will reach there. Now let me select 3 out of 3. We can assume the pattern will hold, \(3 \times 2 \times 1 = 6\) options again. Similarly, 3 options to select 1 out of 3. Notice that, we are only multiplying \(r\) numbers (even though we don’t know what those numbers are) for selecting \(r\) out of \(n\). This is where the equation makes sense. \(n!\) gives us all the ways to select \(n\) out of \(n\).</p> <blockquote> <p>\(x!\) is read as ‘x factorials’. A factorial of a number is defined as \(x! = x \times (x-1) \times (x-2) \times ... \times 1\). Here is an example, \(5! = 5 \times 4 \times 3 \times 2 \times 1 = 120\)</p> </blockquote> <p>Let’s make the example set {ABCDE}. For selecting 5 out of 5, I will have \(5! = 5 \times 4 \times 3 \times 2 \times 1\) options. To select 3 out of those 5, \(5 \times 4 \times 3\) options are available, which do not have \(2 \times 1\) in it, meaning \(5!\) is getting divided by \(2 \times 1 = 2! = (5-3)!\) . There is a simple explanation for this division. Look at the numerator, we are selecting 5 characters when we only wanted 3. And, I have seen in earlier examples with {ABC} that for each first options there are 2 more sub-options. In the numerator, while selecting each 3rd character, instead of stopping, it also has \(2!\) more sub-options and selecting them. Consider an iteration for example, while selecting 3 out of 5 in {ABCDE}, one of the iteration of \(5!\) will select B, select D, select C and then continue on to select E and A. And another iteration will select B, select D, select C and then select A and E. Those sub-options {AE,EA} are extra and I don’t want them in my counting. As those two characters were selected in \(2!\) ways, to remove them, put the same in denominator.</p> <h1 id="combnination">Combnination</h1> <p>Permutation gave us ways to select \(r\) out of \(n\) characters. \(P_{3,3}\) of {ABC} is {ABC, ACB, CAB, BAC, BCA, CBA}. Note that, these are samne characters only in different order. So, order matters in permutation, unlike combination. \(C_\{3,3\}\) of {ABC} is <strong>1</strong>, only {ABC}. How does combination achieve this? From, the equation we see that it first calculates \(P_{n,r}\) and then divides by \(r!\) . In previous section, we learned that to remove some subset of permutation, we divide. The only thing to understand here is which permutations are we removing? For \(C_{3,3}\) , we see that result 1 comes from \(\frac{3!}{3!\ 0!} = \frac{6}{6}\) . It is evident by the same numerator and denominator that what is essentially happening here is, \(\frac{\{ABC, ACB, CAB, BAC, BCA, CBA\}}{\{ABC, ACB, CAB, BAC, BCA, CBA\}}\) . It seems the denominator is duplicates of the parent characters {ABC} (remember, order does not matter here, ABC, BCA is same thing in combination). And how many ways can 2 characters be duplicate, for example, {AB}? That will be \(2!\) ways, {AB,BA}. That is why for \(C_{3,2}\) , \(P_{3,2}\) is divided by \(2!\) . We first get all the ways to select 2 characters out of 3 (which is \(P_{3,2}\) ), and then divide by how many ways 2 characters can be duplicated.</p>]]></content><author><name></name></author><category term="math"/><category term="algorithm"/><category term="theory"/><summary type="html"><![CDATA[explaining permutation and combination with examples.]]></summary></entry><entry><title type="html">Creating emails using EmailMessage in Python 3.8</title><link href="https://riyadhrazzaq.github.io/blog/2020/email-using-python/" rel="alternate" type="text/html" title="Creating emails using EmailMessage in Python 3.8"/><published>2020-09-26T00:01:13+00:00</published><updated>2020-09-26T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/email-using-python</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/email-using-python/"><![CDATA[<h1 id="hello-world-in-email">Hello World in Email</h1> <p>Python has new <code class="language-plaintext highlighter-rouge">EmailMessage</code> class in <code class="language-plaintext highlighter-rouge">email</code> module. Here are a few examples using it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">smtplib</span>
<span class="kn">from</span> <span class="n">email.message</span> <span class="kn">import</span> <span class="n">EmailMessage</span>
<span class="n">sender_email</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sender@mail.com</span><span class="sh">"</span>
<span class="n">receiver_email</span> <span class="o">=</span> <span class="sh">"</span><span class="s">receiver@mail.com</span><span class="sh">"</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">EmailMessage</code> class only creates, manipulates and manages each mail we send, <code class="language-plaintext highlighter-rouge">smtplib</code> is the one that actually sends it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="nf">str</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'\n'
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">str(msg)</code> outputs a readable format of the msg object. For now our message is empty. Let’s add headers. Headers are key, value pair that stores information about our message. For example, <code class="language-plaintext highlighter-rouge">subject</code>, <code class="language-plaintext highlighter-rouge">from</code>, <code class="language-plaintext highlighter-rouge">to</code>, <code class="language-plaintext highlighter-rouge">date</code> etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">this is test subject</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>
<span class="nf">str</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Subject: this is test subject\nTo: receiver@mail.com\nFrom: sender@mail.com\n\n'
</code></pre></div></div> <p>actual content to the msg object is set through the <code class="language-plaintext highlighter-rouge">set_content</code> method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>msg object now looks like this-</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Subject: Hello world in subject
To: receiver@mail.com
From: sender@mail.com
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
MIME-Version: 1.0

Hello world inside message. This is known as message body

</code></pre></div></div> <p>We see 3 extra information. Content-Type is type of the msg body. Content-Transfer-Encoding is how the string in message body is encoded. Existance of <code class="language-plaintext highlighter-rouge">MIME-Version: 1.0</code> indicates that this message maintains MIME (Multipurpose Internet Mail Extensions) standard.</p> <h2 id="mime-in-short">MIME in short</h2> <p>MIME basically says that each message body consists of multiple parts. A message is described as maintype and subtype. Generally written as <code class="language-plaintext highlighter-rouge">maintype/subtype</code>, e.g., <code class="language-plaintext highlighter-rouge">text/plain</code>, <code class="language-plaintext highlighter-rouge">text/html</code>, <code class="language-plaintext highlighter-rouge">application/pdf</code>, <code class="language-plaintext highlighter-rouge">image/png</code> etc. These informations are stored in <code class="language-plaintext highlighter-rouge">Content-Type</code> header that we have seen before. These MIME types let’s us attach images, files, html inside a mail. Since a message body can have multiple combination of these, there should be some MIME type that lets us contain any of these inside the body. Those MIME types are known as <code class="language-plaintext highlighter-rouge">multipart/mixed</code>. So, imagine a tree data structure, root <code class="language-plaintext highlighter-rouge">multipart/mixed</code> will contain concrete types such as <code class="language-plaintext highlighter-rouge">text/plain</code> as nodes, and these nodes will contain the actual message or files as leaves.</p> <h2 id="sending-mail-via-smtplib">Sending Mail via SMTPlib</h2> <p>I will use <code class="language-plaintext highlighter-rouge">mailtrap.io</code> as server, because using gmail or microsoft account and sending numerous emails during development may result in marking the account as spam. SMTP requires connecting to an smtp server via specific host and port, and login if necessary. Then we can send fake emails and see them in <code class="language-plaintext highlighter-rouge">mailtrap.io</code>’s inbox.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>Message sent! <img src="../assets/images/emails-using-python/0.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing.png"/></p> <p>To do this with gmail or microsoft, we just need to change our username, password, host and port with their given configs which is available online.</p> <h1 id="advance-examples">Advance Examples</h1> <h2 id="text-and-image-in-the-body">Text and Image in the body</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">text and image in the body</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">make_mixed</span><span class="p">()</span>

<span class="nf">str</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Subject: text and image in the body\nFrom: sender@mail.com\nTo: receiver@mail.com\nContent-Type: multipart/mixed; boundary="===============4058359157311941825=="\n\n--===============4058359157311941825==\n\n--===============4058359157311941825==--\n'
</code></pre></div></div> <p>Content-type says that our msg is now <code class="language-plaintext highlighter-rouge">multipart</code>. But we need subparts to include inside multipart. <code class="language-plaintext highlighter-rouge">email.message.MIMEPart</code> class can be used to create that. We will need two subparts, one for a sample text, and another for a sample image to display.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">email.message</span> <span class="kn">import</span> <span class="n">MIMEPart</span>
<span class="n">text_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">text_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">plain</span><span class="sh">"</span><span class="p">)</span>

<span class="n">image_filepath</span> <span class="o">=</span> <span class="sh">'</span><span class="s">150.png</span><span class="sh">'</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">img_data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">image_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">image_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="n">img_data</span><span class="p">,</span> <span class="n">maintype</span><span class="o">=</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">png</span><span class="sh">"</span><span class="p">,</span> <span class="n">disposition</span><span class="o">=</span><span class="sh">"</span><span class="s">inline</span><span class="sh">"</span><span class="p">)</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">text_part</span><span class="p">)</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">image_part</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>Result:</p> <p><img src="../assets/images/emails-using-python/1.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing(1).png"/> <img src="../assets/images/emails-using-python/6.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing(6).png"/></p> <p>Those random seeming string is the image as binary placed inline. But not parsed and presented in color. HTML can do that. We will read the image as byte stream and insert that into the <code class="language-plaintext highlighter-rouge">src</code> of html <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tag.</p> <h2 id="image-inline-using-html">Image inline using HTML</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">base64</span>
<span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Image inline with the help of html</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">This will be only shown in text and raw format, HTML won</span><span class="sh">'</span><span class="s">t show this.</span><span class="sh">"</span><span class="p">)</span>

<span class="n">img_data</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">()</span>

<span class="n">html_part</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="se">\
</span><span class="s">&lt;html&gt;
 &lt;body&gt;
    &lt;p&gt;hello world&lt;/p&gt;
   &lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">img_data</span><span class="si">}</span><span class="sh">"</span><span class="s">&gt;
 &lt;/body&gt;
&lt;/html&gt;
</span><span class="sh">"""</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">add_alternative</span><span class="p">(</span><span class="n">html_part</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">html</span><span class="sh">"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>as we see the long line of text via <code class="language-plaintext highlighter-rouge">set_content</code> was not shown. Only the HTML is shown. This type of message is known as <code class="language-plaintext highlighter-rouge">multipart/alternative</code>, where main content will be in plain text and alternative graphically rich version will be put inside html.</p> <p><img src="../assets/images/emails-using-python/3.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing(3).png"/></p> <p>The same thing can be done with <code class="language-plaintext highlighter-rouge">MIMEPart</code> method too.</p> <h2 id="html-as-mimepart">HTML as MIMEPart</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Image inline with the help of html</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">make_mixed</span><span class="p">()</span>

<span class="n">text_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">text_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">plain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">text_part</span><span class="p">)</span>

<span class="n">img_data</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">()</span>

<span class="n">html</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="se">\
</span><span class="s">&lt;html&gt;
 &lt;body&gt;
    &lt;p&gt;hello world&lt;/p&gt;
   &lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">img_data</span><span class="si">}</span><span class="sh">"</span><span class="s">&gt;
 &lt;/body&gt;
&lt;/html&gt;
</span><span class="sh">"""</span>

<span class="n">html_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">html_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">'</span><span class="s">html</span><span class="sh">'</span><span class="p">)</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">html_part</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <h1 id="attach-files">Attach File(s)</h1> <p>File attachment is done using <code class="language-plaintext highlighter-rouge">add_attachment</code> method of msg. But msg has to be <code class="language-plaintext highlighter-rouge">multipart/mixed</code> because only than it can store message in text, html as well as any other file type in its nodes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Image inline with the help of html also has ATTACHMENTS</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">make_mixed</span><span class="p">()</span>

<span class="n">img_data</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">()</span>

<span class="n">html</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="se">\
</span><span class="s">&lt;html&gt;
 &lt;body&gt;
    &lt;p&gt;hello world&lt;/p&gt;
   &lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">img_data</span><span class="si">}</span><span class="sh">"</span><span class="s">&gt;
 &lt;/body&gt;
&lt;/html&gt;
</span><span class="sh">"""</span>

<span class="n">html_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">html_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">'</span><span class="s">html</span><span class="sh">'</span><span class="p">)</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">html_part</span><span class="p">)</span>

<span class="c1"># image attachment: normal file read in python, than create a multipart
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span><span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">only_image</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">add_attachment</span><span class="p">(</span><span class="n">only_image</span><span class="p">,</span> <span class="n">maintype</span><span class="o">=</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">png</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># pdf attachment
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">A Sample PDF.pdf</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pdf_file</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">add_attachment</span><span class="p">(</span><span class="n">pdf_file</span><span class="p">,</span> <span class="n">maintype</span><span class="o">=</span><span class="sh">"</span><span class="s">application</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">octet-stream</span><span class="sh">"</span><span class="p">,</span>
                  <span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">A Small PDF.pdf</span><span class="sh">"</span><span class="p">,</span> <span class="n">disposition</span><span class="o">=</span><span class="sh">"</span><span class="s">attachment</span><span class="sh">"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>So, we have two attachment here also with html body. The second attachment’s filename is shown because we did it manually.</p> <p><img src="../assets/images/emails-using-python/4.png" alt="Email with attachments.png"/></p> <p>Further resources:</p> <ol> <li>https://blog.mailtrap.io/sending-emails-in-python-tutorial-with-code-examples/</li> <li>https://docs.python.org/3/library/email.contentmanager.html#email.contentmanager.ContentManager.set_content</li> <li>https://blog.mailtrap.io/sending-emails-in-python-tutorial-with-code-examples/</li> </ol>]]></content><author><name></name></author><category term="code"/><category term="python"/><summary type="html"><![CDATA[explaining permutation and combination with examples]]></summary></entry><entry><title type="html">ভেক্টর</title><link href="https://riyadhrazzaq.github.io/blog/2020/intro-to-vectors/" rel="alternate" type="text/html" title="ভেক্টর"/><published>2020-08-19T00:01:13+00:00</published><updated>2020-08-19T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/intro-to-vectors</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/intro-to-vectors/"><![CDATA[<h1 id="ভেক্টর-এর-পরিচিতি">ভেক্টর এর পরিচিতি</h1> <p>ভেক্টর এর নির্দিষ্ট সংজ্ঞা শুরুতে দিলে কিছুটা ঝামেলার মনে হতে পারে। একটা উদাহরণ দেই এর চেয়ে। আন্দাজে একটা ভেক্টর ধরে নিলাম, (a=(1,2))। এই ভেক্টর (a) এর মধ্যে দুইজন সদস্য আছে। কথা হচ্ছে এই দুইজন সদস্য কি নির্দেশ করে? <strong>আপাতত উত্তর হল, গ্রাফের মধ্যে একটি বিন্দু</strong>। যেহেতু দুইজন সদস্য তাই একটা 2D গ্রাফে এর অবস্থান। আরেকটা ভেক্টর (b=(2,1)) আনলাম। যেহেতু (a,b) দুজনেরই দুইজন করে সদস্য তারমানে এরা একই গ্রাফে অবস্থান করতে পারে। এখন দেখি আসলে এদেরকে আঁকবো কিভাবে। ছবি ১.১ লক্ষ্য করুন।</p> <p><img src="../assets/images/linear-algebra-ch01/1.png" alt="image"/> <em>ছবি ১.১ গ্রাফের মধ্যে ভেক্টর</em></p> <p>এভাবে সাধারণত ভেক্টর আকা হয়। কিন্তু আমি তো একটু আগে বললাম ভেক্টর হল বিন্দু, তাহলে এখানে লাইন আসলো কই থেকে! <strong>ভেক্টর হল দিক সহ বিন্দু।</strong> দিক নির্দেশ করতে হলে শুরু আর শেষের বিন্দুর প্রয়োজন হয়। যদি শুধু একটা বিন্দু (অনেকগুলো সদস্য) দিয়ে ভেক্টরকে প্রকাশ করা হয়, তাহলে ধরে নেয়া হয় যে শুরুর বিন্দু হচ্ছে ((0,0))। এখন আমি যেহেতু এই ভেক্টর এর সদস্য দুইজন তাই এদেরকে আঁকার জন্য আমার দুইটা <em>axis</em> বা অক্ষ ব্যবহার করতে হয়েছে। আপাতত এটাই ভেক্টর এর পরিচিতি।</p> <h2 id="যোগ">যোগ</h2> <p>উপরের (a,b) ভেক্টর এর যোগ করা যায় সহজেই। এদের সদস্যগুলোকে একটা আরেকটার সাথে যোগ করলেই হয়। মানে, (u = a+b=(3,3))। কিন্তু বিষয় হল এই যোগ কি বুঝায়। দুইটা ভেক্টর এর যোগ হচ্ছে অনেকটা এরকম-</p> <blockquote> <p>একটা ভেক্টর এর শুরুর বিন্দু কে অন্য ভেক্টর এর শেষে নিয়ে যাওয়া।</p> </blockquote> <p>মানে আগে আমরা ((0,0)) কে ভেক্টর এর শুরুর বিন্দু হিসেবে ধরে নিতাম, কিন্তু যোগ করার পর নতুন যে ভেক্টর পাওয়া যায় তা হচ্ছে আগের একটা ভেক্টর কে আরেকটা ভেক্টরের মাথায় নিয়ে যাওয়া। সরানোর পর ভেক্টরটা যে বিন্দুতে পৌঁছাবে, ওইটা হল আমাদের নতুন ভেক্টর, (u)। ছবি ১.২ লক্ষ্য করি।</p> <p><img src="../assets/images/linear-algebra-ch01/2.png" alt="image2"/> <em>ছবি ১.২ ভেক্টর যোগ</em></p> <p>এখানে দুইটির যোগফল হল (u=(3,3))। কিন্তু ডানপাশে ডট ডট আরেকটা ভেক্টর আছে, ওইটা আসলে (a) এবং দেখা যাচ্ছে যে, (a) এর শুরু যদি (b) এর শেষ থেকে হয় তাহলে (a) উভয়ের যোগফলে গিয়ে শেষ হয়। এখানে (b) কে সরিয়ে (a) এর উপর নিলেও একই উত্তর আসবে।</p> <h2 id="গুন">গুন</h2> <p>কোন সংখ্যা দাড়া ভেক্টরকে গুন করলে ওই ভেক্টর শুধু লম্বায় বড় বা ছোট হয়। যেমন, (u = 3a = (3,6))।</p> <p><img src="../assets/images/linear-algebra-ch01/5.png" alt="3a"/></p> <h2 id="নোটেশন">নোটেশন</h2> <p>এতক্ষণ ভেক্টরকে দুইটি ব্র্যাকেটের মধ্যে রাখলেও যেকোনো ভেক্টর, (v) এর আসল রূপ হল এরকম, ) v = \begin{bmatrix} v_1 <br/> v_2 \end{bmatrix} ) শুধুমাত্র লেখকদের সুবিধার জন্য বেশিরভাগ বইয়ে (v=(v_1,v_2)) লেখা হয়।</p>]]></content><author><name></name></author><category term="math"/><category term="theory"/><summary type="html"><![CDATA[explaining permutation and combination with examples.]]></summary></entry><entry><title type="html">Idea and Implementation / Boosting</title><link href="https://riyadhrazzaq.github.io/blog/2020/boosting/" rel="alternate" type="text/html" title="Idea and Implementation / Boosting"/><published>2020-07-27T00:00:00+00:00</published><updated>2020-07-27T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/boosting</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/boosting/"><![CDATA[<h1 id="boosting">Boosting</h1> <p>Boosting is also a general approach that can be applied to many statistical methods. Unlike bagging, boosting use the same dataset. Boosting works on top of weak learners as in very simple algorithms or simple version of a powerful algorithm. For example, a decision tree with 1 split allowed is a weak learner. Output of that tree will be closer to random guessing. Boosting trains the same weak learner multiple times and after each training iterations, updates the dataset.</p> <h2 id="adaboost">AdaBoost</h2> <p>It is a binary classifier where (Y \in {-1, +1}). The algorithm begins with setting up weights (D_i = \frac{1}{N}) for each observation (X_i) where (i=1…N). Suppose we train (T) models, then output from the (t_{th}) model will be (G_t(x_i)). Before the training of (t_{th}) model, we update (X) as (X_i = X_i \times D_i). As we can see, it is element wise operation. This considers as “punishing” the training samples which were misclassified in the previous model. Now that we have punished and trained, we will calculate the error, (err_t)</p> <p>[ \frac{ \sum_{i=1}^{N} D_i \times I(y_i \ne G_t(x_i)) }{\sum_{i=1}^{N} D_i} ]</p> <p>(I(condition)) returns 1 or 0 if the condition is true or false respectively. Now we will have multiple models at our hand and when finally combining their results, we should give more priorities to the more accurate models. This is achieved by calculating, (\alpha_t = \log\left( \frac{1-err_t}{err_t} \right)). Also, we haven’t updated the weights (D_i). They should be updated based on their performance on (t_{th}) tree.</p> <p>[ D_i = D_i \times \exp(\alpha_t \times I(y_i \ne G_t(X_i))) ]</p> <p>using (I(y_i \ne G_t(X_i))) makes sure that we don’t update samples that have been correctly classified. Finally after training of (T) models, we combine their result using majority vote.</p> <p>[ G(x) = sign\left(\sum_{t=1}^{T} \alpha_t \times G_t(x)\right) ]</p> <p>(sign(x)) returns -1 if (x) is negative, 1 if (x) is positive, and 0 if (x) is zero.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdaBoost</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span> <span class="o">=</span> <span class="p">[]</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">nSamples</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nSamples</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">estimator_weights_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">estimator_errors_</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">error_indices</span> <span class="o">=</span> <span class="n">y</span> <span class="o">!=</span> <span class="n">predictions</span>
            <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">[</span><span class="n">error_indices</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>
            
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">err</span><span class="p">)</span> <span class="o">/</span> <span class="n">err</span><span class="p">)</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">[</span><span class="n">error_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">[</span><span class="n">error_indices</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span>
<span class="c1">#             storing meta information
</span>            <span class="n">self</span><span class="p">.</span><span class="n">estimator_weights_</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">estimator_errors_</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nSamples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">Y</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
        
</code></pre></div></div> <h3 id="trying-out-the-model">Trying out the model</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">1</span><span class="p">,.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,.</span><span class="mi">08</span><span class="p">,.</span><span class="mi">12</span><span class="p">,.</span><span class="mi">33</span><span class="p">,.</span><span class="mi">55</span><span class="p">,.</span><span class="mi">66</span><span class="p">,.</span><span class="mi">77</span><span class="p">,.</span><span class="mi">88</span><span class="p">,.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">5</span><span class="p">,.</span><span class="mi">6</span><span class="p">,.</span><span class="mi">25</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">5</span><span class="p">,.</span><span class="mi">7</span><span class="p">,.</span><span class="mi">6</span><span class="p">])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">65</span><span class="p">,.</span><span class="mi">7</span><span class="p">,.</span><span class="mi">6</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">1</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">66</span><span class="p">,.</span><span class="mi">77</span><span class="p">,.</span><span class="mi">65</span><span class="p">,.</span><span class="mi">68</span><span class="p">,.</span><span class="mi">55</span><span class="p">,.</span><span class="mi">44</span><span class="p">,.</span><span class="mi">1</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">15</span><span class="p">,.</span><span class="mi">15</span><span class="p">,.</span><span class="mi">5</span><span class="p">,.</span><span class="mi">55</span><span class="p">,.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)).</span><span class="n">T</span>

<span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">==-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div> <p>sklearn’s</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">boost</span> <span class="o">=</span> <span class="nc">AdaBoostClassifier</span><span class="p">(</span> <span class="n">base_estimator</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> 
                            <span class="n">algorithm</span> <span class="o">=</span> <span class="sh">'</span><span class="s">SAMME</span><span class="sh">'</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">boost</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">boost</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.8695652173913043
</code></pre></div></div> <p>ours</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ab</span> <span class="o">=</span> <span class="nc">AdaBoost</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ab</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">ab</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.8695652173913043
</code></pre></div></div> <p>nice!</p> <h1 id="references">References</h1> <ol> <li>Freund, Yoav, and Robert E. Schapire. “A desicion-theoretic generalization of on-line learning and an application to boosting.” European conference on computational learning theory. Springer, Berlin, Heidelberg, 1995.</li> <li>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media, 2009.</li> <li>James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li> </ol>]]></content><author><name></name></author><category term="mahcine-learning"/><summary type="html"><![CDATA[Boosting Boosting is also a general approach that can be applied to many statistical methods. Unlike bagging, boosting use the same dataset. Boosting works on top of weak learners as in very simple algorithms or simple version of a powerful algorithm. For example, a decision tree with 1 split allowed is a weak learner. Output of that tree will be closer to random guessing. Boosting trains the same weak learner multiple times and after each training iterations, updates the dataset. AdaBoost It is a binary classifier where (Y \in {-1, +1}). The algorithm begins with setting up weights (D_i = \frac{1}{N}) for each observation (X_i) where (i=1…N). Suppose we train (T) models, then output from the (t_{th}) model will be (G_t(x_i)). Before the training of (t_{th}) model, we update (X) as (X_i = X_i \times D_i). As we can see, it is element wise operation. This considers as “punishing” the training samples which were misclassified in the previous model. Now that we have punished and trained, we will calculate the error, (err_t)]]></summary></entry><entry><title type="html">Idea and Implementation / Random Forest</title><link href="https://riyadhrazzaq.github.io/blog/2020/random-forest/" rel="alternate" type="text/html" title="Idea and Implementation / Random Forest"/><published>2020-07-26T00:00:00+00:00</published><updated>2020-07-26T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/random-forest</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/random-forest/"><![CDATA[<h1 id="random-forest">Random Forest</h1> <p>RF is similar to bagging with one difference. While training each of \(B\) datasets, bagging considers all the features \(p_i\), RF does not. This is a way to decorrelate individual models. RF is mostly use with decision tree and the idea behind it seems clear and intuitive in that case.</p> <p>In normal situation, one tree is grown with all features. And in these features, some might be strong predictors. Therefore, while growing multiple trees, most of them will use the strong features in their top split. Hence, the predictions will be highly correlated and our goal to reduce variance will be slightly derailed.</p> <p>A key property in RF is the choice of \(m\), the number of features to consider each split. If we use this feature subsets, then on average, \((p-m)/p\) splits will not even consider the strong predictors, and others will. This will decorrelate the trees, and the average of the predictions will be less variable.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RF</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Implements RF from scratch with scikit learn</span><span class="sh">'</span><span class="s">s DecisionTreeRegressor.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        params
        ------
        B: int.
            Number of trees in the forest.
        
        m: int.
            Number of features to consider in each tree.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)</span> <span class="c1"># max_features is sklearn's 'm' variable
</span>            <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nSamples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">Y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span>
        <span class="k">return</span> <span class="n">y</span>        
</code></pre></div></div> <h2 id="trying-out-the-models">Trying out the models</h2> <p>sklearn’s</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">sk</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">sk</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">sk</span><span class="p">.</span><span class="nf">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([2.27887587])
</code></pre></div></div> <p>ours</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rfr</span> <span class="o">=</span> <span class="nc">RF</span><span class="p">(</span><span class="n">B</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">rfr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">rfr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([3.20605521])
</code></pre></div></div> <h1 id="references">References</h1> <ol> <li>Freund, Yoav, and Robert E. Schapire. “A desicion-theoretic generalization of on-line learning and an application to boosting.” European conference on computational learning theory. Springer, Berlin, Heidelberg, 1995.</li> <li>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media, 2009.</li> <li>James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Random Forest]]></summary></entry><entry><title type="html">Idea and Implementation / Bagging</title><link href="https://riyadhrazzaq.github.io/blog/2020/bagging/" rel="alternate" type="text/html" title="Idea and Implementation / Bagging"/><published>2020-07-25T00:00:00+00:00</published><updated>2020-07-25T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/bagging</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/bagging/"><![CDATA[<p>Bagging, random forest, and boosting are statistical approach to further enhance already available algorithms. They all seem to deal with multiple training of a models on the samples from one dataset. Let’s look at the idea behind these.</p> <h1 id="bagging">Bagging</h1> <p>Consider a decision tree. When I split a dataset, and fit two distinct decision tree on those two halves, it will give different outputs. Meaning, our models will have high variance. Bagging or Bootstrap Aggragation comes from this problem. It is a general purpose approach for reducing the variance of models.</p> <p>Theoretically, averaging samples reduces the variance. So, to solve the problem above, we can just train multiple models, get their predictions, average them, and voila! This is what bagging is. But how can we get multiple dataset though? We use <a href="https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29">bootstrap</a> for that. Bootstrap will generate B different training datasets from one single dataset. We will train on \(b_{th}\) dataset to get \(f_b(x)\). When producing prediction, we will do average of all the \(f_b(x)\). For a single sample \(x\), the equation stands,</p> \[\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^{B} f_b(x)\] <h2 id="implementation-in-python">Implementation in Python</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span><span class="p">,</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="n">importlib</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Bagging</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">modelName</span><span class="o">=</span><span class="sh">'</span><span class="s">sklearn.linear_model.LinearRegression</span><span class="sh">'</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        params
        ------
        B: int. 
            B separate datasets will be generated.
        
        modelName: str. 
            A sklearn Estimator for regression. Module name and model name must be separated by dot(.). Default: </span><span class="sh">"</span><span class="s">sklearn.linear_model.LinearRegressor</span><span class="sh">"</span><span class="s">
        
        returns
        -------
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelName</span> <span class="o">=</span> <span class="n">modelName</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Fits B model to B datasets.
        
        params
        ------
        X, y: typical input ndarray.
        </span><span class="sh">"""</span>
        <span class="n">nSamples</span><span class="p">,</span> <span class="n">nFeats</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">importlib</span><span class="p">.</span><span class="nf">import_module</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">modelName</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">modelName</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="nf">eval</span><span class="p">(</span><span class="sh">'</span><span class="s">self.module.</span><span class="sh">'</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">modelName</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="sh">"</span><span class="s">()</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">],</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Averages outputs from B model to predict each observations.
        
        params
        ------
        X: typical input ndarray.
        </span><span class="sh">"""</span>
        <span class="n">nSamples</span><span class="p">,</span> <span class="n">nFeats</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nSamples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">))</span> <span class="c1"># output for each model is a column vector. output for each sample is row vector
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">Y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span>
        
        <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div> <h2 id="trying-out-the-model">Trying out the model</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                       <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_targets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((100, 4), (100,))
</code></pre></div></div> <p>sklearn’s model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">regr</span> <span class="o">=</span> <span class="nc">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="nc">LinearRegression</span><span class="p">(),</span>
                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">regr</span><span class="p">.</span><span class="nf">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.54321e-15])
</code></pre></div></div> <p>Our Model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bag</span> <span class="o">=</span> <span class="nc">Bagging</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">bag</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">bag</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([6.99440506e-16])
</code></pre></div></div> <p>Nice!</p> <h1 id="references">References</h1> <ol> <li>Freund, Yoav, and Robert E. Schapire. “A desicion-theoretic generalization of on-line learning and an application to boosting.” European conference on computational learning theory. Springer, Berlin, Heidelberg, 1995.</li> <li>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media, 2009.</li> <li>James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Bagging, random forest, and boosting are statistical approach to further enhance already available algorithms. They all seem to deal with multiple training of a models on the samples from one dataset. Let’s look at the idea behind these.]]></summary></entry><entry><title type="html">Idea and Implementation / Multinomial Naive Bayes</title><link href="https://riyadhrazzaq.github.io/blog/2020/multi-naive-bayes/" rel="alternate" type="text/html" title="Idea and Implementation / Multinomial Naive Bayes"/><published>2020-07-05T00:00:00+00:00</published><updated>2020-07-05T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/multi-naive-bayes</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/multi-naive-bayes/"><![CDATA[<p>Multinomial naive bayes is the naive Bayes algorithm for multinomially distributed data. For a brief and intuitive explanation of Bayes theorem, read this kernel of mine: <a href="https://www.kaggle.com/riyadhrazzaq/gaussian-naive-bayes-classifier">Gaussian Naive Bayes Classifier from Scratch</a>. Everything is similar to Gaussian NB except the \(P(x_i \mid y)\). The new equation is, \(P(x_i \mid y) = \frac{N_{yi} + \alpha}{N_y + \alpha n} \label{eq1}\tag{1}\) Here,</p> <ul> <li>\(\alpha\) is the smoothing parameter,</li> <li>\(N_{yi}\) is the count of feature \(x_i\) in class y.</li> <li>\(N_y\) is the total count of all features in class y</li> <li>\(n\) is the total number of features</li> </ul> <h1 id="multinomial-naive-bayes">Multinomial Naive Bayes</h1> <p>You can look up in detail about multinomial distribution and you should. I will only put a short description of how a multinomial naive bayes classifier considers data.</p> <h2 id="multinomial-data">Multinomial Data</h2> <table> <thead> <tr> <th>\(X_1\)</th> <th>\(X_2\)</th> <th>\(X_3\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0</td> <td>4</td> </tr> <tr> <td>4</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <p>In the table above containing 2 sample of 3 features, we observe that feature \(X_1\) has values 1 and 4, and so on. That is the common view of the data. And when other a general model accepts this data, it considers each number as value. For example, \(X_{1,2}=3\). But in case of reading a multinomial data, \(X_{1,2}\) says how many of feature \(X_{2}\) is in sample 1. Meaning \(X_{1,2}\) is not value of the feature, instead it is the count of the feature. Let’s consider a text corpus. Each sentence is made up of different words \(w_i\) and each of those \(w_i\) belongs to the vocabulary, \(V\). If \(V\) contains 8 words, \(w_1,w_2,...,w_8\) and if a sentence is: w1 w2 w2 w6 w3 w2 w8, the representation of that sentence will be-</p> <table> <thead> <tr> <th>\(w_1\)</th> <th>\(w_2\)</th> <th>\(w_3\)</th> <th>\(w_4\)</th> <th>\(w_5\)</th> <th>\(w_6\)</th> <th>\(w_7\)</th> <th>\(w_8\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>3</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>1</td> </tr> </tbody> </table> <p>After inserting some other random sentences, the dataset is-</p> <table> <thead> <tr> <th>\(w_1\)</th> <th>\(w_2\)</th> <th>\(w_3\)</th> <th>\(w_4\)</th> <th>\(w_5\)</th> <th>\(w_6\)</th> <th>\(w_7\)</th> <th>\(w_8\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>3</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>1</td> </tr> <tr> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> <td>3</td> </tr> <tr> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>2</td> <td>1</td> <td>2</td> </tr> </tbody> </table> <p>By the way, I haven’t put them in a class. Randomly taking, \(y\) = [1,0,1]. Now, comparing with the equation of above,</p> <ul> <li>\(N_{yi}\) is the count of feature \(w_i\) in each unique class of y. For example, for \(y=1\), \(N_{y,1}=1, N_{y,6}=3\)</li> <li>\(N_y\) is the total count of all features in each unique class of y. For example, for \(y=1\), \(N_y=12\)</li> <li>\(n=8\) is the total number of features</li> <li>\(\alpha\) is known as smoothing parameter. It is needed for zero probability problem which is explained in resource [1]</li> </ul> <p>To calculate likelyhoods for a test sentence, all we need is \(P(w_i \mid y)\) which will be used to calculate \(P(X \mid y)\) from training data. But \(P(w_i \mid y)\) is the probability of feature \(w_i\) appearing under class y once. If our test sentence has any feature \(w_i\) n times, we will need to include \(P(w_i \mid y)\) in \(P(X \mid y)\) n times too. So, final equation for \(P(X_i \mid y)\) will be-</p> \[P(X_i \mid y) = P(w_1 \mid y)^{X_{i,1}} \times P(w_2 \mid y)^{X_{i,2}} \times ... \times P(w_n \mid y)^{X_{i,n}}\] <p>Resources:</p> <ol> <li>https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf</li> <li>https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes</li> </ol> <h3 id="some-libraries-and-test-data">Some libraries and test data</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span> 
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># test data
</span><span class="n">tmpX1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="sh">"</span><span class="s">2   0   0   0   1   2   3   1  0   0   1   0   2   1   0   0  0   1   0   1   0   2   1   0  1   0   0   2   0   1   0   1  2   0   0   0   1   0   1   3  0   0   1   2   0   0   2   1</span><span class="sh">"</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">  </span><span class="sh">"</span><span class="p">)])</span>
<span class="n">tmpX2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="sh">"</span><span class="s">0   1   1   0   0   0   1   0  1   2   0   1   0   0   1   1  0   1   1   0   0   2   0   0  0   0   0   0   0   0   0   0  0   0   1   0   1   0   1   0</span><span class="sh">"</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">  </span><span class="sh">"</span><span class="p">)])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">tmpX1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">tmpX2</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">X and Y shapes</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X and Y shapes
 (11, 8) (11,)
</code></pre></div></div> <h1 id="class-multinb">Class MultiNB</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiNB</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    
    <span class="k">def</span> <span class="nf">_prior</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># CHECKED
</span>        <span class="sh">"""</span><span class="s">
        Calculates prior for each unique class in y. P(y)
        </span><span class="sh">"""</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_samples</span>
        <span class="k">return</span> <span class="n">P</span>
            
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="c1"># CHECKED, matches with sklearn
</span>        <span class="sh">"""</span><span class="s">
        Calculates the following things- 
            class_priors_ is list of priors for each y.
            N_yi: 2D array. Contains for each class in y, the number of time each feature i appears under y.
            N_y: 1D array. Contains for each class in y, the number of all features appear under y.
            
        params
        ------
        X: 2D array. shape(n_samples, n_features)
            Multinomial data
        y: 1D array. shape(n_samples,). Labels must be encoded to integers.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">class_priors_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_prior</span><span class="p">()</span>
        
        <span class="c1"># distinct values in each features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">uniques</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">):</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">uniques</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">tmp</span> <span class="p">)</span>
            
        <span class="n">self</span><span class="p">.</span><span class="n">N_yi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">))</span> <span class="c1"># feature count
</span>        <span class="n">self</span><span class="p">.</span><span class="n">N_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span> <span class="c1"># total count 
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span> <span class="c1"># x axis
</span>            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
            <span class="n">columnwise_sum</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">):</span> <span class="c1"># y axis
</span>                <span class="n">columnwise_sum</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span><span class="n">j</span><span class="p">]))</span>
                
            <span class="n">self</span><span class="p">.</span><span class="n">N_yi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">columnwise_sum</span> <span class="c1"># 2d
</span>            <span class="n">self</span><span class="p">.</span><span class="n">N_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">columnwise_sum</span><span class="p">)</span> <span class="c1"># 1d
</span>            
    <span class="k">def</span> <span class="nf">_theta</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Calculates theta_yi. aka P(xi | y) using eqn(1) in the notebook.
        
        params
        ------
        x_i: int. 
            feature x_i
            
        i: int.
            feature index. 
            
        h: int or string.
            a class in y
        
        returns
        -------
        theta_yi: P(xi | y)
        </span><span class="sh">"""</span>
        
        <span class="n">Nyi</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">N_yi</span><span class="p">[</span><span class="n">h</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
        <span class="n">Ny</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">N_y</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
        
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">Nyi</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">Ny</span> <span class="o">+</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">)</span>
        
        <span class="nf">return  </span><span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">)</span><span class="o">**</span><span class="n">x_i</span>
    
    <span class="k">def</span> <span class="nf">_likelyhood</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Calculates P(E|H) = P(E1|H) * P(E2|H) .. * P(En|H).
        
        params
        ------
        x: array. shape(n_features,)
            a row of data.
        h: int. 
            a class in y
        </span><span class="sh">"""</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">tmp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_theta</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span><span class="n">h</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">predict_proba</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">samples</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">joint_likelyhood</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span>
            
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">):</span>
                <span class="n">joint_likelyhood</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">class_priors_</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">_likelyhood</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">h</span><span class="p">)</span> <span class="c1"># P(y) P(X|y) 
</span>                
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">joint_likelyhood</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">):</span>
                <span class="n">numerator</span> <span class="o">=</span> <span class="n">joint_likelyhood</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
                <span class="n">self</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">)</span>
            
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Sklearn Sanity Check
    </span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">,</span><span class="sh">'</span><span class="s">Sklearn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sk_y</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Count </span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">feature_count_</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Class Log Prior </span><span class="sh">"</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">class_log_prior_</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy </span><span class="sh">'</span><span class="p">,</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">sk_y</span><span class="p">),</span><span class="n">sk_y</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">,</span><span class="sh">'</span><span class="s">Custom</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">nb</span> <span class="o">=</span> <span class="nc">MultiNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">nb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">nb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">me_score</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Count</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">nb</span><span class="p">.</span><span class="n">N_yi</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Class Log Prior </span><span class="sh">"</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">class_priors_</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy </span><span class="sh">'</span><span class="p">,</span><span class="n">me_score</span><span class="p">,</span><span class="n">yhat</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">)</span> <span class="c1"># my predict proba is only for last test set
</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">pipeline</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-------------------- Sklearn --------------------
Feature Count 
 [[5. 1. 2. 5. 4. 6. 7. 6.]
 [1. 4. 3. 1. 1. 2. 3. 1.]]
Class Log Prior  [-0.6061358  -0.78845736]
Accuracy  0.8181818181818182 [0 0 0 0 0 0 1 1 1 0 0]
[[0.74940942 0.25059058]
 [0.52879735 0.47120265]
 [0.53711475 0.46288525]
 [0.69613326 0.30386674]
 [0.75239818 0.24760182]
 [0.62207341 0.37792659]
 [0.39213534 0.60786466]
 [0.45705923 0.54294077]
 [0.42055705 0.57944295]
 [0.54545455 0.45454545]
 [0.51099295 0.48900705]]
-------------------- Custom --------------------
Feature Count
 [[5. 1. 2. 5. 4. 6. 7. 6.]
 [1. 4. 3. 1. 1. 2. 3. 1.]]
Class Log Prior  [-0.6061358  -0.78845736]
Accuracy  0.8181818181818182 [0 0 0 0 0 0 1 1 1 0 0]
[[0.74940942 0.25059058]
 [0.52879735 0.47120265]
 [0.53711475 0.46288525]
 [0.69613326 0.30386674]
 [0.75239818 0.24760182]
 [0.62207341 0.37792659]
 [0.39213534 0.60786466]
 [0.45705923 0.54294077]
 [0.42055705 0.57944295]
 [0.54545455 0.45454545]
 [0.51099295 0.48900705]]
</code></pre></div></div> <h1 id="spam-classification">Spam Classification</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">import</span> <span class="n">string</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/spam_uci.csv</span><span class="sh">"</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">iso8859_14</span><span class="sh">'</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <h2 id="simple-preprocessing">Simple Preprocessing</h2> <p>to cleanup punctuations and stopwords</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">clean_util</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">punc_rmv</span> <span class="o">=</span> <span class="p">[</span><span class="n">char</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">]</span>
    <span class="n">punc_rmv</span> <span class="o">=</span> <span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">punc_rmv</span><span class="p">)</span>
    <span class="n">stopword_rmv</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">punc_rmv</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">w</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)]</span>
    
    <span class="k">return</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">stopword_rmv</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="n">clean_util</span><span class="p">)</span>
</code></pre></div></div> <h1 id="vectorizing">Vectorizing</h1> <p>Conforming the texts to the multinomial format we have discussed in the beginning. Also, classes in y must be converted to integers as I forgot to account for strings in my implementation and too lazy to update •͡˘㇁•͡˘</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cv</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cv</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]).</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">lb</span> <span class="o">=</span> <span class="nc">LabelBinarizer</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lb</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]).</span><span class="nf">ravel</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(5572, 9381) (5572,)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train Test Split
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(4179, 9381) (1393, 9381) (4179,) (1393,)
</code></pre></div></div> <p>sklearn’s <code class="language-plaintext highlighter-rouge">MultinomialNB</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sk</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">sk</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9755922469490309
</code></pre></div></div> <p>our <code class="language-plaintext highlighter-rouge">MultiNB</code> (⌐■_■)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">me</span> <span class="o">=</span> <span class="nc">MultiNB</span><span class="p">()</span>
<span class="n">me</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">me</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">yhat</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9755922469490309
CPU times: user 1min 5s, sys: 0 ns, total: 1min 5s
Wall time: 1min 5s
</code></pre></div></div> <p>It takes a lot of time but does not matter as it is a reference implementation only ヽ(｀Д´)ﾉ</p> <p><strong>I wrote the scratch implementation for my learning, if you see any error or typo, please let me know.</strong></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Multinomial naive bayes is the naive Bayes algorithm for multinomially distributed data. For a brief and intuitive explanation of Bayes theorem, read this kernel of mine: Gaussian Naive Bayes Classifier from Scratch. Everything is similar to Gaussian NB except the \(P(x_i \mid y)\). The new equation is, \(P(x_i \mid y) = \frac{N_{yi} + \alpha}{N_y + \alpha n} \label{eq1}\tag{1}\) Here, \(\alpha\) is the smoothing parameter, \(N_{yi}\) is the count of feature \(x_i\) in class y. \(N_y\) is the total count of all features in class y \(n\) is the total number of features]]></summary></entry><entry><title type="html">Idea and Implementation / Bayes Theorem and Gaussian Naive Bayes</title><link href="https://riyadhrazzaq.github.io/blog/2020/gauss-naive-bayes/" rel="alternate" type="text/html" title="Idea and Implementation / Bayes Theorem and Gaussian Naive Bayes"/><published>2020-07-03T00:01:13+00:00</published><updated>2020-07-03T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/gauss-naive-bayes</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/gauss-naive-bayes/"><![CDATA[<h1 id="bayes-theroem">Bayes Theroem</h1> <p>Any naive bayes approach including Gaussian Naive Bayes depends on the Bayes Theorem. Bayes theorem gives us the probability of an event, given that we have some extra knowledge about that event.</p> \[P(A \mid B) = \frac{P(A) \ P(B \mid A)}{P(B)}\] <p>This is the mathmatical definition of the theorem. Here, \(A\) and \(B\) are events and \(P(B) \ne 0\)</p> <ul> <li>\(P(A \mid B)\) a conditional probability. It is read as the likelyhood of event A occuring given that event B is true. In an experiment this is our objective variable.</li> <li>\(P(A)\) is prior probability. We are supposed to observe the value of this in our experiment.</li> <li>\(P(B \mid A)\) is the conditional probability of B happening given A true.</li> </ul> <p>There is a rather intuitive explanation behind this legendary theroy.</p> <h2 id="smoke-and-fire-experiment">Smoke and Fire Experiment</h2> <p>Our hypothesis is that there will be fire if there is smoke. So, we are asking for \(P(Fire \mid Smoke)\). To answer that we’ll start with an experiment sample.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/images/2020-07-03-gauss-naive-bayes/NB1-480.webp 480w,/assets/images/2020-07-03-gauss-naive-bayes/NB1-800.webp 800w,/assets/images/2020-07-03-gauss-naive-bayes/NB1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/images/2020-07-03-gauss-naive-bayes/NB1.png" class="img-fluid z-depth-1" width="100%" height="auto" style=" max-width: 800; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When we collect data about <em>Fire</em> and <em>No Fire</em> events, the area of the white square represents the total sample space.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/images/2020-07-03-gauss-naive-bayes/NB2-480.webp 480w,/assets/images/2020-07-03-gauss-naive-bayes/NB2-800.webp 800w,/assets/images/2020-07-03-gauss-naive-bayes/NB2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/images/2020-07-03-gauss-naive-bayes/NB2.png" class="img-fluid z-depth-1" width="100%" height="auto" style=" max-width: 800; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And we see that we area on the left is <em>Fire</em> and on the right is <em>No Fire</em> samples.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/images/2020-07-03-gauss-naive-bayes/NB3-480.webp 480w,/assets/images/2020-07-03-gauss-naive-bayes/NB3-800.webp 800w,/assets/images/2020-07-03-gauss-naive-bayes/NB3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/images/2020-07-03-gauss-naive-bayes/NB3.png" class="img-fluid z-depth-1" width="100%" height="auto" style=" max-width: 800; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now let’s observe in which samples smoke was seen in a fire, and in which sample smoke was seen even though there were no fire. Note that, smoke seen in a fire can be written as \((Smoke \mid Fire)\) and not seen in a fire can be written as \((Smoke \mid No Fire)\). Red areas represents smoke was seen. These red areas that I have drawn is based on gut feeling. Smoke is common when there’s a fire, so red area is bigger compared to the red area when there is no fire. Now, as we know probability of something happening is effectively the ratio of that thing against all other thing. Hence, \(P(Smoke \mid Fire)\) is equals to red area inside fire divided by total red are in experiment.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/images/2020-07-03-gauss-naive-bayes/NB5-480.webp 480w,/assets/images/2020-07-03-gauss-naive-bayes/NB5-800.webp 800w,/assets/images/2020-07-03-gauss-naive-bayes/NB5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/images/2020-07-03-gauss-naive-bayes/NB5.png" class="img-fluid z-depth-1" width="100%" height="auto" style=" max-width: 800; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>But how the red areas can be calculated mathematically? Well, area is the product of height and width. In our experiment, height is the \(P(Smoke \mid Fire)\) and width is \(P(Fire)\). Similarly the red area in <em>No Fire</em> can be calculated.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/images/2020-07-03-gauss-naive-bayes/NB6-480.webp 480w,/assets/images/2020-07-03-gauss-naive-bayes/NB6-800.webp 800w,/assets/images/2020-07-03-gauss-naive-bayes/NB6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/images/2020-07-03-gauss-naive-bayes/NB6.png" class="img-fluid z-depth-1" width="100%" height="auto" style=" max-width: 800; " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So, we finally get this equation on top.</p> <p>Note: This is just a geometrical explanation of Bayes Theorem. Gaussian Naive Bayes and other naive bayes algorithms differentiate themselves from this by how they calculate \(P(Smoke \mid Fire)\).</p> <h1 id="gaussian-naive-bayes">Gaussian Naive Bayes</h1> <p>This algorithm assumes likelyhoods of features are of gaussian distribution.</p> \[P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)\] <h1 id="implementation">Implementation</h1> <p>We need to calculate priors,\(P(A)\), mean \(\mu_y\) and standard deviation, \(\sigma_y\) of all features for each available classes in y. Then a function to crunch the above formula of \(P(x_i \mid y)\)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1"># linear algebra
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span> <span class="c1"># data processing, CSV file I/O (e.g. pd.read_csv)
</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GaussNB</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        No params are needed for basic functionality.
        </span><span class="sh">"""</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">_mean</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span> <span class="c1"># CHECKED
</span>        <span class="sh">"""</span><span class="s">
        Returns class probability for each 
        </span><span class="sh">"""</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_feats</span><span class="p">):</span>
                <span class="n">mean</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="p">))</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="k">return</span> <span class="n">mu</span>
    
    <span class="k">def</span> <span class="nf">_stddev</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span> <span class="c1"># CHECKED
</span>        <span class="n">sigma</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
            <span class="n">stddev</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_feats</span><span class="p">):</span>
                <span class="n">stddev</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="n">j</span><span class="p">])</span> <span class="p">)</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">stddev</span>
        <span class="k">return</span> <span class="n">sigma</span>
    
    <span class="k">def</span> <span class="nf">_prior</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># CHECKED
</span>        <span class="sh">"""</span><span class="s">Prior probability, P(y) for each y
        </span><span class="sh">"""</span>
        <span class="n">P</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">).</span><span class="nf">flatten</span><span class="p">().</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">probability</span> <span class="o">=</span> <span class="n">count</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability</span>
        <span class="k">return</span> <span class="n">P</span>
    
    <span class="k">def</span> <span class="nf">_normal</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mean</span><span class="p">,</span><span class="n">stddev</span><span class="p">):</span> <span class="c1"># CHECKED
</span>        <span class="sh">"""</span><span class="s">
        Gaussian Normal Distribution
        $P(x_i \mid y) = </span><span class="se">\f</span><span class="s">rac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-</span><span class="se">\f</span><span class="s">rac{(x_i - \mu_y)^2}{2\sigma^2_y}</span><span class="se">\r</span><span class="s">ight)$
        </span><span class="sh">"""</span>
        
        <span class="n">multiplier</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">stddev</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span> 
        <span class="n">exp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">stddev</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">multiplier</span> <span class="o">*</span> <span class="n">exp</span>

    
    <span class="k">def</span> <span class="nf">P_E_H</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Uses Normal Distribution to get, P(E|H) = P(E1|H) * P(E2|H) .. * P(En|H)
        
        params
        ------
        X: 1dim array. 
            E in P(E|H)
        H: class in y
        </span><span class="sh">"""</span>
        <span class="n">pdfs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_feats</span><span class="p">):</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">means_</span><span class="p">[</span><span class="n">h</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">stddevs_</span><span class="p">[</span><span class="n">h</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">pdfs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">_normal</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">)</span> <span class="p">)</span>
            
        <span class="n">p_e_h</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">pdfs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p_e_h</span>
        
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_feats</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">means_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># dict of list {class:feats}
</span>        <span class="n">self</span><span class="p">.</span><span class="n">stddevs_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_stddev</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># dict of list {class:feat}
</span>        <span class="n">self</span><span class="p">.</span><span class="n">priors_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_prior</span><span class="p">()</span> <span class="c1"># dict of priors 
</span>        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">feats</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">samples</span><span class="o">!=</span><span class="n">self</span><span class="p">.</span><span class="n">n_samples</span> <span class="ow">or</span> <span class="n">feats</span><span class="o">!=</span><span class="n">self</span><span class="p">.</span><span class="n">n_feats</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">DimensionError</span><span class="p">(</span><span class="sh">"</span><span class="s">No dimension match with training data!</span><span class="sh">"</span><span class="p">)</span>
            
        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
            <span class="n">distinct_likelyhoods</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">P_E_H</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">h</span><span class="p">)</span>
                <span class="n">distinct_likelyhoods</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">tmp</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">priors_</span><span class="p">[</span><span class="n">h</span><span class="p">])</span>
            <span class="n">marginal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">distinct_likelyhoods</span><span class="p">)</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">probas</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span>
                <span class="n">numerator</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">priors_</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">*</span> <span class="n">distinct_likelyhoods</span><span class="p">[</span><span class="n">tmp</span><span class="p">]</span>
                <span class="n">denominator</span> <span class="o">=</span> <span class="n">marginal</span>
                <span class="n">probas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span> <span class="p">)</span>
                <span class="n">tmp</span><span class="o">+=</span><span class="mi">1</span>
            <span class="c1"># predicting maximum
</span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gnb</span> <span class="o">=</span> <span class="nc">GaussianNB</span><span class="p">()</span>
<span class="n">sk_pred</span> <span class="o">=</span> <span class="n">gnb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">).</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sci-kit Learn: </span><span class="sh">"</span><span class="p">,</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">sk_pred</span><span class="p">))</span>

<span class="n">nb</span> <span class="o">=</span> <span class="nc">GaussNB</span><span class="p">()</span>
<span class="n">nb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">me_pred</span> <span class="o">=</span> <span class="n">nb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Custom GaussNB: </span><span class="sh">"</span><span class="p">,</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">me_pred</span><span class="p">))</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine-learning"/><category term="algorithm"/><category term="theory"/><summary type="html"><![CDATA[visual explanation of Naive Bayes, inspired by 3Blue1Brown.]]></summary></entry></feed>