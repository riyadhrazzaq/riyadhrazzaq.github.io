<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://riyadhrazzaq.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://riyadhrazzaq.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-07T09:04:43+00:00</updated><id>https://riyadhrazzaq.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Multimodal LLMs on top of LLaMA 3</title><link href="https://riyadhrazzaq.github.io/blog/2024/nov24-papers/" rel="alternate" type="text/html" title="Multimodal LLMs on top of LLaMA 3"/><published>2024-11-30T00:00:00+00:00</published><updated>2024-11-30T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/nov24-papers</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/nov24-papers/"><![CDATA[<p>Research works based on LLaMA 3 <a class="citation" href="#dubey_llama_2024">(Dubey et al., 2024)</a> have been popping up since its release in July. I have been focused on multimodal papers based on LLaMA 3 for my project. One of the primary paper I am focused on right now is the LLaMA-Omni <a class="citation" href="#fang_omni">(Fang et al., 2024)</a>. It adds the speech modality on top of the text already in Llama-3.1-8B Instruct model. Their work fills the void of GPT-4o’s <a class="citation" href="#openai_gpt-4o_2024">(OpenAI et al., 2024)</a> capability of end-to-end speech interaction in the sphere of open-source LLMs. In the simplest term, they wanted the model to accept speech and generate speech as output. The authors uses a novel technique to achieve this goal. At first, the speech \(X^s\) passes through a frozen speech encoder, in this case, Whisper V3 <a class="citation" href="#whisperv3">(Radford et al., 2023)</a>. Whisper takes 30s frames of log-melspectrogram and outputs a hidden representation \(\vec{H} = [\vec{h_1}, … \vec{h_N}]\). These \(\vec{H}\) are downsampled by concatenating every \(k\) column vectors which gives \(\vec{H}'\) with length \(\frac{N}{k}\) instead of \(N\). Finally, this downsampled represenation goes through a speech adaptor (a typical feed forward neural netowork) that basically maps these to the LLM. The text and speech instructions are combined for the LLM, where the text is fixed for all samples and only the speech varies. You can see the template below.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are a helpful language and speech assistant. You are able to understand the speech content that the user provides, and assist the user with a variety of tasks using natural language.
&lt;speech&gt;
Please answer the questions in the user’s input speech.
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-30-nov24-papers/llama-omni-480.webp 480w,/assets/img/2024-11-30-nov24-papers/llama-omni-800.webp 800w,/assets/img/2024-11-30-nov24-papers/llama-omni-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-11-30-nov24-papers/llama-omni.png" class="img-fluid z-depth-1" width="100%" height="auto" title="llama-omni architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> LLaMA-Omni architecture <a class="citation" href="#fang_omni">(Fang et al., 2024)</a> </div> <p>The <code class="language-plaintext highlighter-rouge">&lt;speech&gt;</code> is the \(\vec{H}'\). The LLM auto-regressively generates the output text as all LLaMA models do and trained with cross-entropy. To generate the speech directly, the output hidden state for the LLM are fed to a speech decoder as each tokens are predicted continuously. Note that even though output hidden states are fed through the speech decoder sequentially, one token after another, there is no hidden state that are being shared, making this a non-autoregressive technique. This makes generating the speech almost as fast as the text generation, differentiating this approach from a cascading technique. The speech decoder produces speech units that are basically scalers in \([1, K]\) range which a vocoder uses to produce audio. During infernece, the system waits for a few speech units and then generates a partial audio from them, thus, producing a streaming audio.</p> <p>LLASM <a class="citation" href="#shu_llasm_2023">(Shu et al., 2023)</a> is another model take understands speech + text to produce text only output. The core of this paper evolves around aligning speech embedding and text embedding together, which it does by going through a separate pre-training phase for a speech adaptor. The Whisper <a class="citation" href="#whisperv3">(Radford et al., 2023)</a> encodes the audio to produce speech embeddings and how the text embeddings are produced not mentioned in the paper. But that seems like a trivial point in 2024.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-30-nov24-papers/llasm-480.webp 480w,/assets/img/2024-11-30-nov24-papers/llasm-800.webp 800w,/assets/img/2024-11-30-nov24-papers/llasm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-11-30-nov24-papers/llasm.png" class="img-fluid z-depth-1" width="100%" height="auto" title="llasm architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> LLaSM architecture <a class="citation" href="#shu_llasm_2023">(Shu et al., 2023)</a> </div> <p>Duirng the pre-training, the encoder and the LLM is frozen. The adaptor is trained with an ASR task where the instruction is a text, for example, “Transcribe the following speech into text”. There are many similar instructions in Chinese and English. The adaptor takes audio embeddings from Whisper. They are fed to the adaptor together in an interleaved manner as the LLaMA-Omni paper. This technique of interleaving is getting quite popular. The LLM and the adaptor is trained again the next phase: cross-modal instruction fine-tuning, which is basically multi-task training with cross-entropy. One of the primary achievement of this paper is creating the cross-modal instruction fine-tuning dataset.</p> <h1 id="similarities">Similarities</h1> <p>I am noticing that recent works are moving away from the cascading nature of transcribing the speech with an ASR first and also applying TTS for producing audio. Interleaving embeddings are the next hot-topic in this field.</p> <h3 id="references">References</h3> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="dubey_llama_2024" class="col-sm-8"> <div class="title">The Llama 3 Herd of Models</div> <div class="author"> Abhimanyu Dubey,&nbsp;Abhinav Jauhri,&nbsp;Abhinav Pandey, and <span class="more-authors" title="click to view 532 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '532 more authors' ? 'Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao' : '532 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">532 more authors</span> </div> <div class="periodical"> Aug 2024 </div> <div class="periodical"> arXiv:2407.21783 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2407.21783" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> <div class="abstract hidden"> <p>Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="fang_omni" class="col-sm-8"> <div class="title">LLaMA-Omni: Seamless Speech Interaction with Large Language Models</div> <div class="author"> Qingkai Fang,&nbsp;Shoutao Guo,&nbsp;Yan Zhou, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhengrui Ma, Shaolei Zhang, Yang Feng' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Sep 2024 </div> <div class="periodical"> arXiv:2409.06666 </div> <div class="links"> <a href="https://doi.org/10.48550/arXiv.2409.06666" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="openai_gpt-4o_2024" class="col-sm-8"> <div class="title">GPT-4o System Card</div> <div class="author"> OpenAI,&nbsp;Aaron Hurst,&nbsp;Adam Lerer, and <span class="more-authors" title="click to view 416 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '416 more authors' ? 'Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, A. J. Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov' : '416 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">416 more authors</span> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> arXiv:2410.21276 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2410.21276" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> <div class="abstract hidden"> <p>GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It’s trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50}% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o’s capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we’ve implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o’s text and vision capabilities.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="whisperv3" class="col-sm-8"> <div class="title">Robust speech recognition via large-scale weak supervision</div> <div class="author"> Alec Radford,&nbsp;Jong Wook Kim,&nbsp;Tao Xu, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Greg Brockman, Christine McLeavey, Ilya Sutskever' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, Honolulu, Hawaii, USA, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="shu_llasm_2023" class="col-sm-8"> <div class="title">LLaSM: Large Language and Speech Model</div> <div class="author"> Yu Shu,&nbsp;Siwei Dong,&nbsp;Guangyao Chen, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, Yemin Shi' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> Sep 2023 </div> <div class="periodical"> arXiv:2308.15930 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2308.15930" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> <div class="abstract hidden"> <p>Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions dataset is available at https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.</p> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="paper"/><category term="nlp"/><summary type="html"><![CDATA[notes on paper read in Nov, 24.]]></summary></entry><entry><title type="html">Paper Notes / Contrastive Predicting Coding</title><link href="https://riyadhrazzaq.github.io/blog/2024/contrastive-predictive-coding/" rel="alternate" type="text/html" title="Paper Notes / Contrastive Predicting Coding"/><published>2024-11-15T00:01:13+00:00</published><updated>2024-11-15T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/contrastive-predictive-coding</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/contrastive-predictive-coding/"><![CDATA[<p>This is an attempt to write down my understanding of the paper “<a href="http://arxiv.org/abs/1807.03748">Representation Learning with Contrastive Predictive Coding” by Oord et al.</a></p> <h1 id="ultimate-goal">Ultimate Goal</h1> <p>The primary goal is to learn speech representation that can be further used on numerous downstream tasks such as ASR, ST etc. Now, language models typically learn this sort of representation through next token prediction. The model produces a latent representation from which the output distribution for the next token is generated. The loss function is usually a cross-entropy function.</p> <p>This approach is not suitable for speech because the speech signal is high dimensional whether it is raw wave signals or log mel-spectrograms. And high dimensional signals do not produce strong results for such a task.</p> <h1 id="the-re-oriented-approach">The Re-oriented Approach</h1> <p>Assume we have continuous signals \(\vec{X} = \\{\vec{x_1},...,\vec{x_M}\\}\). Each \(x_i\) is a vector of signal representation. Given \(x_t\), we will now predict the latent representation for \(x_{t+k}\), where \(k &gt; t\). This is presented in the paper in the following way: instead of modelling \(p(\mathbf{x_{t+k} | c_t})\), this approach will model \(f_k(\mathbf{x_{t+k}, c_t}) = exp(\mathbf{z_{t+k}^T \cdot W_k \cdot c_t})\) which is a density ratio between the context at \(t^{th}\) step to the latent representation at \(t+k^{th}\) step. Here, \(c_t\) is the context representation and \(z_t\) is the latent representation. In some way, we are looking for the relationship between the two and our loss function will reflect that more clearly.</p> <h1 id="training">Training</h1> <h2 id="forward">Forward</h2> <p>We have an encoder layer that will generate latent representation \(\mathbf{z_t}\) and another auto-regressive layer which will generate \(\mathbf{c_t}\) from \(\mathbf{z_t}\) and \(\mathbf{c_{p &lt; t}}\).</p> <h2 id="backward">backward</h2> <p>Instead of directly predicting the latent representation, the model will learn to contrast between the latent representations. We want the predicted latent representation for the current time step to be similar to the \(k^{th}\) tokens after it and different from other representations. We call them positive and negative samples respectively.</p> <p>The model is given \(N\) samples among which \(N-1\) are negative samples. The positive sample is drawn from a regular conditional model \(p(\mathbf{x_{t+k}|c_t})\) (this can be an n-gram model). The negative samples are drawn from \(p(\mathbf{x_{t+k}})\) - which is an overall collection of all the possible values (like the vocab in a text LLM). \(\begin{align*} L_N &amp;= - \Bbb{E} log \frac{f_k(\mathbf{x_{t+k}, c_t})}{\sum_{x_j \in \mathbf{X}} f_k(\mathbf{x_j, c_t}) } \\ &amp;= - \Bbb{E} log \frac{exp(\mathbf{z_{t+k}^T W_k c_t})}{\sum_{x_j \in \mathbf{X}} exp(\mathbf{z_j^T W_k c_t})} \\ &amp;= - \Bbb{E} log \frac{cosine\ similarity\ between\ current\ context\ repr.\ and\ future\ true\ latent\ repr.}{sum\ of\ the\ cosine\ similarities\ between\ all\ the\ pos \&amp; neg\ future\ samples} \end{align*}\)</p> <p>This loss penalizes the model if the cosine similarity between true latent representation is larger compared to all the cosine similarity it calculated The only way for the model to perform well is to learn to contrast between the positive sample and the other \(N-1\) negative samples.</p> <h2 id="some-comments">Some Comments</h2> <p>In the loss function, \(W_k\) is different for each \(k\). So, we can swap the weights afterwards if we want the model to perform the prediction for the next step or \(k\) steps after that. Even though we are sampling \(N\) samples, whether be it positive or negative, the loss is calculated from the latent representation of that \(\mathbf{x}\)</p> <h1 id="references">References</h1> <ol> <li>Oord, A. van den, Li, Y., &amp; Vinyals, O. (2019). Representation Learning with Contrastive Predictive Coding (arXiv:1807.03748). arXiv. http://arxiv.org/abs/1807.03748</li> <li>E. Hinton, G. (2013). LEARNING DISTRIBUTED REPRESENTATIONS FOR STATISTICAL LANGUAGE MODELLING. Retrieved November 16, 2024, from http://www.cs.utoronto.ca/%7Ehinton/csc2535/notes/hlbl.pdf</li> </ol>]]></content><author><name></name></author><category term="machine-learning"/><category term="paper"/><category term="speech"/><summary type="html"><![CDATA[my notes on the paper 'Representation Learning with Contrastive Predictive Coding' by Oord et al.]]></summary></entry><entry><title type="html">আমার ইরাস্মুসে আবেদন</title><link href="https://riyadhrazzaq.github.io/blog/2024/lct/" rel="alternate" type="text/html" title="আমার ইরাস্মুসে আবেদন"/><published>2024-08-23T00:01:13+00:00</published><updated>2024-08-23T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/lct</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/lct/"><![CDATA[<p>২০২২ এ GRE ও IELTS পরীক্ষা দেই। পরেরটায় আলহামদুলিল্লাহ ভালো ফলাফল আসলেও প্রথমটায় একদমই খারাপ করি। নিচে আমার শিক্ষা জীবনের ফলাফলের সারাংশ দেয়া হলঃ</p> <blockquote> <p>এস এস সিঃ ৫.০০ <br/> এইচ এস সিঃ ৫.০০ <br/> ব্যাচেলরঃ ৩.৭০ ইউনিভার্সিটি অফ এশিয়া প্যাসিফিক থেকে <br/> IELTS: 7.5 (L9, R8, W7, S6.5)</p> </blockquote> <p>GRE তে খারাপ করার পর মানসিকভাবে ভেঙে পরি। যদিও আমার তখনকার উদ্দ্যেশ্য ছিল আমেরিকায় পি এইচ ডি প্রোগ্রামে আবেদন করা, কিন্তু ঐ পরিস্থিতিতে আমি আত্মবিশ্বাস রাখতে পারি নাই। কিছুদিন হাহুতাশ করার পর সিদ্ধান্ত নেই ইরাস্মুস প্রোগ্রামে আবেদন করবো। নভেম্বরের শেষে এই সিদ্ধান্ত নেয়ার পেছনে শুধু একটাই কারন ছিল, আমি পরের বছর আমেরিকায় আবেদন করার আগে একবার SOP লেখা এবং উচ্চ শিক্ষায় আবেদনের পুরো প্রক্রিয়ার একটা অভিজ্ঞতা নিতে চাচ্ছিলাম।</p> <p>প্রথম ধাপেই আমি ইরাস্মুস প্রোগ্রামের তালিকা থেকে নিজের পছন্দসই ৫ টা নির্বাচন করি। ইরাস্মুসে সীমাহীন প্রোগ্রামে আবেদন করা গেলেও, বাস্তবে তা করা যায় না। কারন সবগুলো প্রোগ্রামই আলাদা বৈচিত্র্যের, তাই আলাদা SOP লিখতে হবে। প্রোগ্রাম নির্বাচনের ক্ষেত্রে আমি নিম্নোক্ত বিষয়গুলোতে গুরুত্ব দেইঃ</p> <p>১। সব মাস্টার্স প্রোগ্রামই কোন নির্দিষ্ট বিষয়ে গুরুত্ব দিয়ে থাকে, যেমনঃ AI নিয়ে প্রোগ্রাম না করে, NLP কিংবা Medical AI নিয়ে প্রোগ্রাম বানায়। নিজের আগ্রহ এবং পূর্ববর্তী গবেষণার সাথে মিল থাকে এমন প্রোগ্রাম নির্বাচন করা উত্তম। <br/> ২। ইন্টার্নশিপ আছে নাকি নাই সেটাও খেয়াল করি। আমি ইন্টার্নশিপওয়ালা প্রোগ্রামে গুরুত্ব দেই বেশি, কারন এতে পরে চাকরি পেতে সুবিধা হবে বলে ধারনা করি। <br/> ৩। সাধারণত প্রোগ্রামের ওয়েবসাইটে এরা IELTS অথবা GRE নিয়ে কিছু বলে না। যদি বলে তবে আমি ঐ প্রোগ্রাম নির্বাচন করি, কারন IELTS এ ভালো করায় আমি ধরে নেই আমি অন্যান্য আবেদনকারীদের থেকে সামান্য হলেও এগিয়ে থাকবো (যদিও কতটুকু সত্য সেটা আমি জানি না, পুরোটাই আন্দাজ)।</p> <p>এইসকল বিষয় বিবেচনায় আমি EDISS, LCT, SECCLO, GENIAL, MAIA প্রোগ্রামগুলোর জন্য প্রস্তুতি নেই। প্রস্তুতি হিসেবে শুরুতেই কি কি কাগজপত্র লাগবে, SOP এর জন্য কিছু প্রোগ্রাম আলাদা নির্দেশনা দেয়, সেগুলো টুকে নেই। Recommendation Letter আরেকটি গুরুত্বপূর্ণ কিন্তু বিরক্তিকর বিষয়। আমার নির্বাচিত সব প্রোগ্রামেই ২ জনের সুপারিশ পত্র লেগেছিল। তাদেরকে আবেদনের সিদ্ধান্ত নেয়ার সাথে সাথেই অনুরোধ করে রাখি।</p> <p>আমি শুরুতে একটি SOP লিখি শুধু EDISS প্রোগ্রামের জন্য। দেড় পৃষ্ঠার ঐ লেখায় প্রথম অনুচ্ছেদে নিজের কম্পিউটার বিজ্ঞানে আগ্রহ কিভাবে আসে তা বলি। তারপরের অনুচ্ছেদে ব্যাচেলরে নিজের পড়াশোনা ও ফলাফল ছোট করে বিবৃতি দেই, তারপর জানাই কিভাবে NLP তে আগ্রহ এলো। আরেকটা অনুচ্ছেদে লিখি চাকুরীজীবনে কিসব দায়িত্ব নিয়ে কাজ করেছি। সর্বশেষ অনুচ্ছেদে এই প্রোগ্রামের সাথে নিজের আগ্রহের সম্পর্ক এবং এই প্রোগ্রাম যে আমাকে সাহায্য করবে কর্মজীবনের উদ্দেশ্য সাধনে তা বলি।</p> <p>তারপর এই SOP আমার বউ (উনিও বিদেশে উচ্চ শিক্ষায় গিয়েছেন) এবং আমার এক শিক্ষককে দেখাই। তাদের অনেক উপদেশের পর একটি গ্রহণযোগ্য খসড়া প্রস্তুত হয় এবং আমি সেটা অন্য প্রোগ্রামগুলোর জন্য পরিবর্তন করে নেই। এভাবে একমাস লাগিয়ে আমার SOP লেখা শেষ হয়। SOP একবার লিখেই শেষ করে দেয়া যায় না, যতবারই পড়ি ততবারই কিছু না কিছু হালকা পরিবর্তনের কথা মাথায় আসে। তাই SOP বেশ আগে থেকেই লেখা শুরু করা উচিত।</p> <p>সুপারিশপত্র বেশিরভাগ ক্ষেত্রেই শিক্ষক লিখে দেন। যদি নিজের লিখতে হয় সেক্ষেত্রে যতটা ব্যক্তিগত খাতিরের কথা লেখা যায় তত ভালো। খাতির মানে এই না যে উনি আপনাকে পছন্দ করতো সেটা লিখবেন, বরং শ্রেণীকক্ষে আপনার কি ধরনের কাজের জন্য আপনাকে উনি আপনাকে সুপারিশপত্র দিতে রাজি হয়েছে তা লিখবেন। আপনি শ্রেণীকক্ষে মনযোগী ছিলেন, কোন বিশেষ ল্যাব কিংবা এসাইনমেন্টে আপনার কাজ খুবই ভালো হয়েছিল এগুলা বলতে পারেন।</p> <p>বেশিরভাগ প্রোগ্রামেই আপনার CV চাইবে, সেক্ষেত্রে কর্মজীবনের আর শিক্ষাজীবনের CV আলাদা হয় সেটা লক্ষ্য রাখবেন। CV তে যায়গা সঙ্কট হলে প্রজেক্ট/পেপারের এর বিবরণ সংক্ষেপিত করা যায়। আবেদনের শেষ সীমার বেশ আগেই আবেদন করার চেষ্টা করবেন যাতে সুপারিশদাতারা যথেষ্ট সময় পান।</p> <p>২ মাস পর ফলাফল দিলে আমাকে দুই প্রোগ্রামে অপেক্ষমাণ তালিকায় রাখে। পরিশেষে LCT প্রোগ্রাম থেকে ডাক দেয়।</p>]]></content><author><name></name></author><category term="academia"/><category term="scholarship"/><summary type="html"><![CDATA[how I applied for the Erasmus Mundus scholarship.]]></summary></entry><entry><title type="html">Permutation and Combination</title><link href="https://riyadhrazzaq.github.io/blog/2024/counting/" rel="alternate" type="text/html" title="Permutation and Combination"/><published>2024-01-04T00:01:13+00:00</published><updated>2024-01-04T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2024/counting</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2024/counting/"><![CDATA[<p>Most of us are already introduced to permutation and combination in high school math classes. I understand clearly what they separately define. But I get confused easily between their equations because they never made sense to me. This post is my attempt to make sense of the following equations.<br/> \(\begin{align*} P_{n,r} &amp;= \frac{n!}{(n-r)!} \\ C_{n,r} &amp;= \frac{n!}{(n-r)!\ r!} \end{align*}\)</p> <h1 id="permutation">Permutation</h1> <p>I will attempt to figure out the number of ways to select 2 characters out of 3. The characters are {ABC}, \(n=3, r=2\). How many ways can I select 2 out of 3 characters? The answer is given by the permutation equation. Let me break it down to <em>options</em>. During the first selection, I have 3 options {ABC}, which will leave me 2 options for the second and for the final selection only 1 option will be left. As I get 3 options for the first, and for each of those first options, I get 2 sub-options, I have a total of \(3 \times 2 = 6\) options. However, it is not close to the \(P_{n,r}\) equation, but we will reach there. Now let me select 3 out of 3. We can assume the pattern will hold, \(3 \times 2 \times 1 = 6\) options again. Similarly, 3 options to select 1 out of 3. Notice that, we are only multiplying \(r\) numbers (even though we don’t know what those numbers are) for selecting \(r\) out of \(n\). This is where the equation makes sense. \(n!\) gives us all the ways to select \(n\) out of \(n\).</p> <blockquote> <p>\(x!\) is read as ‘x factorials’. A factorial of a number is defined as \(x! = x \times (x-1) \times (x-2) \times ... \times 1\). Here is an example, \(5! = 5 \times 4 \times 3 \times 2 \times 1 = 120\)</p> </blockquote> <p>Let’s make the example set {ABCDE}. For selecting 5 out of 5, I will have \(5! = 5 \times 4 \times 3 \times 2 \times 1\) options. To select 3 out of those 5, \(5 \times 4 \times 3\) options are available, which do not have \(2 \times 1\) in it, meaning \(5!\) is getting divided by \(2 \times 1 = 2! = (5-3)!\) . There is a simple explanation for this division. Look at the numerator, we are selecting 5 characters when we only wanted 3. And, I have seen in earlier examples with {ABC} that for each first options there are 2 more sub-options. In the numerator, while selecting each 3rd character, instead of stopping, it also has \(2!\) more sub-options and selecting them. Consider an iteration for example, while selecting 3 out of 5 in {ABCDE}, one of the iteration of \(5!\) will select B, select D, select C and then continue on to select E and A. And another iteration will select B, select D, select C and then select A and E. Those sub-options {AE,EA} are extra and I don’t want them in my counting. As those two characters were selected in \(2!\) ways, to remove them, put the same in denominator.</p> <h1 id="combnination">Combnination</h1> <p>Permutation gave us ways to select \(r\) out of \(n\) characters. \(P_{3,3}\) of {ABC} is {ABC, ACB, CAB, BAC, BCA, CBA}. Note that, these are samne characters only in different order. So, order matters in permutation, unlike combination. \(C_\{3,3\}\) of {ABC} is <strong>1</strong>, only {ABC}. How does combination achieve this? From, the equation we see that it first calculates \(P_{n,r}\) and then divides by \(r!\) . In previous section, we learned that to remove some subset of permutation, we divide. The only thing to understand here is which permutations are we removing? For \(C_{3,3}\) , we see that result 1 comes from \(\frac{3!}{3!\ 0!} = \frac{6}{6}\) . It is evident by the same numerator and denominator that what is essentially happening here is, \(\frac{\{ABC, ACB, CAB, BAC, BCA, CBA\}}{\{ABC, ACB, CAB, BAC, BCA, CBA\}}\) . It seems the denominator is duplicates of the parent characters {ABC} (remember, order does not matter here, ABC, BCA is same thing in combination). And how many ways can 2 characters be duplicate, for example, {AB}? That will be \(2!\) ways, {AB,BA}. That is why for \(C_{3,2}\) , \(P_{3,2}\) is divided by \(2!\) . We first get all the ways to select 2 characters out of 3 (which is \(P_{3,2}\) ), and then divide by how many ways 2 characters can be duplicated.</p>]]></content><author><name></name></author><category term="math"/><category term="algorithm"/><category term="theory"/><summary type="html"><![CDATA[explaining permutation and combination with examples.]]></summary></entry><entry><title type="html">Creating emails using EmailMessage in Python 3.8</title><link href="https://riyadhrazzaq.github.io/blog/2020/email-using-python/" rel="alternate" type="text/html" title="Creating emails using EmailMessage in Python 3.8"/><published>2020-09-26T00:01:13+00:00</published><updated>2020-09-26T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/email-using-python</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/email-using-python/"><![CDATA[<h1 id="hello-world-in-email">Hello World in Email</h1> <p>Python has new <code class="language-plaintext highlighter-rouge">EmailMessage</code> class in <code class="language-plaintext highlighter-rouge">email</code> module. Here are a few examples using it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">smtplib</span>
<span class="kn">from</span> <span class="n">email.message</span> <span class="kn">import</span> <span class="n">EmailMessage</span>
<span class="n">sender_email</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sender@mail.com</span><span class="sh">"</span>
<span class="n">receiver_email</span> <span class="o">=</span> <span class="sh">"</span><span class="s">receiver@mail.com</span><span class="sh">"</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">EmailMessage</code> class only creates, manipulates and manages each mail we send, <code class="language-plaintext highlighter-rouge">smtplib</code> is the one that actually sends it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="nf">str</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'\n'
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">str(msg)</code> outputs a readable format of the msg object. For now our message is empty. Let’s add headers. Headers are key, value pair that stores information about our message. For example, <code class="language-plaintext highlighter-rouge">subject</code>, <code class="language-plaintext highlighter-rouge">from</code>, <code class="language-plaintext highlighter-rouge">to</code>, <code class="language-plaintext highlighter-rouge">date</code> etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">this is test subject</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>
<span class="nf">str</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Subject: this is test subject\nTo: receiver@mail.com\nFrom: sender@mail.com\n\n'
</code></pre></div></div> <p>actual content to the msg object is set through the <code class="language-plaintext highlighter-rouge">set_content</code> method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>msg object now looks like this-</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Subject: Hello world in subject
To: receiver@mail.com
From: sender@mail.com
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
MIME-Version: 1.0

Hello world inside message. This is known as message body

</code></pre></div></div> <p>We see 3 extra information. Content-Type is type of the msg body. Content-Transfer-Encoding is how the string in message body is encoded. Existance of <code class="language-plaintext highlighter-rouge">MIME-Version: 1.0</code> indicates that this message maintains MIME (Multipurpose Internet Mail Extensions) standard.</p> <h2 id="mime-in-short">MIME in short</h2> <p>MIME basically says that each message body consists of multiple parts. A message is described as maintype and subtype. Generally written as <code class="language-plaintext highlighter-rouge">maintype/subtype</code>, e.g., <code class="language-plaintext highlighter-rouge">text/plain</code>, <code class="language-plaintext highlighter-rouge">text/html</code>, <code class="language-plaintext highlighter-rouge">application/pdf</code>, <code class="language-plaintext highlighter-rouge">image/png</code> etc. These informations are stored in <code class="language-plaintext highlighter-rouge">Content-Type</code> header that we have seen before. These MIME types let’s us attach images, files, html inside a mail. Since a message body can have multiple combination of these, there should be some MIME type that lets us contain any of these inside the body. Those MIME types are known as <code class="language-plaintext highlighter-rouge">multipart/mixed</code>. So, imagine a tree data structure, root <code class="language-plaintext highlighter-rouge">multipart/mixed</code> will contain concrete types such as <code class="language-plaintext highlighter-rouge">text/plain</code> as nodes, and these nodes will contain the actual message or files as leaves.</p> <h2 id="sending-mail-via-smtplib">Sending Mail via SMTPlib</h2> <p>I will use <code class="language-plaintext highlighter-rouge">mailtrap.io</code> as server, because using gmail or microsoft account and sending numerous emails during development may result in marking the account as spam. SMTP requires connecting to an smtp server via specific host and port, and login if necessary. Then we can send fake emails and see them in <code class="language-plaintext highlighter-rouge">mailtrap.io</code>’s inbox.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>Message sent! <img src="../assets/images/emails-using-python/0.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing.png"/></p> <p>To do this with gmail or microsoft, we just need to change our username, password, host and port with their given configs which is available online.</p> <h1 id="advance-examples">Advance Examples</h1> <h2 id="text-and-image-in-the-body">Text and Image in the body</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">text and image in the body</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">make_mixed</span><span class="p">()</span>

<span class="nf">str</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Subject: text and image in the body\nFrom: sender@mail.com\nTo: receiver@mail.com\nContent-Type: multipart/mixed; boundary="===============4058359157311941825=="\n\n--===============4058359157311941825==\n\n--===============4058359157311941825==--\n'
</code></pre></div></div> <p>Content-type says that our msg is now <code class="language-plaintext highlighter-rouge">multipart</code>. But we need subparts to include inside multipart. <code class="language-plaintext highlighter-rouge">email.message.MIMEPart</code> class can be used to create that. We will need two subparts, one for a sample text, and another for a sample image to display.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">email.message</span> <span class="kn">import</span> <span class="n">MIMEPart</span>
<span class="n">text_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">text_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">plain</span><span class="sh">"</span><span class="p">)</span>

<span class="n">image_filepath</span> <span class="o">=</span> <span class="sh">'</span><span class="s">150.png</span><span class="sh">'</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">img_data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">image_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">image_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="n">img_data</span><span class="p">,</span> <span class="n">maintype</span><span class="o">=</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">png</span><span class="sh">"</span><span class="p">,</span> <span class="n">disposition</span><span class="o">=</span><span class="sh">"</span><span class="s">inline</span><span class="sh">"</span><span class="p">)</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">text_part</span><span class="p">)</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">image_part</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>Result:</p> <p><img src="../assets/images/emails-using-python/1.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing(1).png"/> <img src="../assets/images/emails-using-python/6.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing(6).png"/></p> <p>Those random seeming string is the image as binary placed inline. But not parsed and presented in color. HTML can do that. We will read the image as byte stream and insert that into the <code class="language-plaintext highlighter-rouge">src</code> of html <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tag.</p> <h2 id="image-inline-using-html">Image inline using HTML</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">base64</span>
<span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Image inline with the help of html</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">This will be only shown in text and raw format, HTML won</span><span class="sh">'</span><span class="s">t show this.</span><span class="sh">"</span><span class="p">)</span>

<span class="n">img_data</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">()</span>

<span class="n">html_part</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="se">\
</span><span class="s">&lt;html&gt;
 &lt;body&gt;
    &lt;p&gt;hello world&lt;/p&gt;
   &lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">img_data</span><span class="si">}</span><span class="sh">"</span><span class="s">&gt;
 &lt;/body&gt;
&lt;/html&gt;
</span><span class="sh">"""</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">add_alternative</span><span class="p">(</span><span class="n">html_part</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">html</span><span class="sh">"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>as we see the long line of text via <code class="language-plaintext highlighter-rouge">set_content</code> was not shown. Only the HTML is shown. This type of message is known as <code class="language-plaintext highlighter-rouge">multipart/alternative</code>, where main content will be in plain text and alternative graphically rich version will be put inside html.</p> <p><img src="../assets/images/emails-using-python/3.png" alt="Screenshot_2020-09-26 Mailtrap - Safe Email Testing(3).png"/></p> <p>The same thing can be done with <code class="language-plaintext highlighter-rouge">MIMEPart</code> method too.</p> <h2 id="html-as-mimepart">HTML as MIMEPart</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Image inline with the help of html</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">make_mixed</span><span class="p">()</span>

<span class="n">text_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">text_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">plain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">text_part</span><span class="p">)</span>

<span class="n">img_data</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">()</span>

<span class="n">html</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="se">\
</span><span class="s">&lt;html&gt;
 &lt;body&gt;
    &lt;p&gt;hello world&lt;/p&gt;
   &lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">img_data</span><span class="si">}</span><span class="sh">"</span><span class="s">&gt;
 &lt;/body&gt;
&lt;/html&gt;
</span><span class="sh">"""</span>

<span class="n">html_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">html_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">'</span><span class="s">html</span><span class="sh">'</span><span class="p">)</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">html_part</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <h1 id="attach-files">Attach File(s)</h1> <p>File attachment is done using <code class="language-plaintext highlighter-rouge">add_attachment</code> method of msg. But msg has to be <code class="language-plaintext highlighter-rouge">multipart/mixed</code> because only than it can store message in text, html as well as any other file type in its nodes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msg</span> <span class="o">=</span> <span class="nc">EmailMessage</span><span class="p">()</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">Subject</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Image inline with the help of html also has ATTACHMENTS</span><span class="sh">"</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">To</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">receiver_email</span>
<span class="n">msg</span><span class="p">[</span><span class="sh">'</span><span class="s">From</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender_email</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">make_mixed</span><span class="p">()</span>

<span class="n">img_data</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64encode</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">()).</span><span class="nf">decode</span><span class="p">()</span>

<span class="n">html</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="se">\
</span><span class="s">&lt;html&gt;
 &lt;body&gt;
    &lt;p&gt;hello world&lt;/p&gt;
   &lt;img src=</span><span class="sh">"</span><span class="s">data:image/png;base64,</span><span class="si">{</span><span class="n">img_data</span><span class="si">}</span><span class="sh">"</span><span class="s">&gt;
 &lt;/body&gt;
&lt;/html&gt;
</span><span class="sh">"""</span>

<span class="n">html_part</span> <span class="o">=</span> <span class="nc">MIMEPart</span><span class="p">()</span>
<span class="n">html_part</span><span class="p">.</span><span class="nf">set_content</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">'</span><span class="s">html</span><span class="sh">'</span><span class="p">)</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">attach</span><span class="p">(</span><span class="n">html_part</span><span class="p">)</span>

<span class="c1"># image attachment: normal file read in python, than create a multipart
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">,</span><span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">only_image</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">add_attachment</span><span class="p">(</span><span class="n">only_image</span><span class="p">,</span> <span class="n">maintype</span><span class="o">=</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">png</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># pdf attachment
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">A Sample PDF.pdf</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pdf_file</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="n">msg</span><span class="p">.</span><span class="nf">add_attachment</span><span class="p">(</span><span class="n">pdf_file</span><span class="p">,</span> <span class="n">maintype</span><span class="o">=</span><span class="sh">"</span><span class="s">application</span><span class="sh">"</span><span class="p">,</span> <span class="n">subtype</span><span class="o">=</span><span class="sh">"</span><span class="s">octet-stream</span><span class="sh">"</span><span class="p">,</span>
                  <span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">A Small PDF.pdf</span><span class="sh">"</span><span class="p">,</span> <span class="n">disposition</span><span class="o">=</span><span class="sh">"</span><span class="s">attachment</span><span class="sh">"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">smtplib</span><span class="p">.</span><span class="nc">SMTP</span><span class="p">(</span><span class="sh">"</span><span class="s">smtp.mailtrap.io</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2525</span><span class="p">)</span> <span class="k">as</span> <span class="n">server</span><span class="p">:</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">login</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">)</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <p>So, we have two attachment here also with html body. The second attachment’s filename is shown because we did it manually.</p> <p><img src="../assets/images/emails-using-python/4.png" alt="Email with attachments.png"/></p> <p>Further resources:</p> <ol> <li>https://blog.mailtrap.io/sending-emails-in-python-tutorial-with-code-examples/</li> <li>https://docs.python.org/3/library/email.contentmanager.html#email.contentmanager.ContentManager.set_content</li> <li>https://blog.mailtrap.io/sending-emails-in-python-tutorial-with-code-examples/</li> </ol>]]></content><author><name></name></author><category term="code"/><category term="python"/><summary type="html"><![CDATA[explaining permutation and combination with examples]]></summary></entry><entry><title type="html">ভেক্টর</title><link href="https://riyadhrazzaq.github.io/blog/2020/intro-to-vectors/" rel="alternate" type="text/html" title="ভেক্টর"/><published>2020-08-19T00:01:13+00:00</published><updated>2020-08-19T00:01:13+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/intro-to-vectors</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/intro-to-vectors/"><![CDATA[<h1 id="ভেক্টর-এর-পরিচিতি">ভেক্টর এর পরিচিতি</h1> <p>ভেক্টর এর নির্দিষ্ট সংজ্ঞা শুরুতে দিলে কিছুটা ঝামেলার মনে হতে পারে। একটা উদাহরণ দেই এর চেয়ে। আন্দাজে একটা ভেক্টর ধরে নিলাম, (a=(1,2))। এই ভেক্টর (a) এর মধ্যে দুইজন সদস্য আছে। কথা হচ্ছে এই দুইজন সদস্য কি নির্দেশ করে? <strong>আপাতত উত্তর হল, গ্রাফের মধ্যে একটি বিন্দু</strong>। যেহেতু দুইজন সদস্য তাই একটা 2D গ্রাফে এর অবস্থান। আরেকটা ভেক্টর (b=(2,1)) আনলাম। যেহেতু (a,b) দুজনেরই দুইজন করে সদস্য তারমানে এরা একই গ্রাফে অবস্থান করতে পারে। এখন দেখি আসলে এদেরকে আঁকবো কিভাবে। ছবি ১.১ লক্ষ্য করুন।</p> <p><img src="../assets/images/linear-algebra-ch01/1.png" alt="image"/> <em>ছবি ১.১ গ্রাফের মধ্যে ভেক্টর</em></p> <p>এভাবে সাধারণত ভেক্টর আকা হয়। কিন্তু আমি তো একটু আগে বললাম ভেক্টর হল বিন্দু, তাহলে এখানে লাইন আসলো কই থেকে! <strong>ভেক্টর হল দিক সহ বিন্দু।</strong> দিক নির্দেশ করতে হলে শুরু আর শেষের বিন্দুর প্রয়োজন হয়। যদি শুধু একটা বিন্দু (অনেকগুলো সদস্য) দিয়ে ভেক্টরকে প্রকাশ করা হয়, তাহলে ধরে নেয়া হয় যে শুরুর বিন্দু হচ্ছে ((0,0))। এখন আমি যেহেতু এই ভেক্টর এর সদস্য দুইজন তাই এদেরকে আঁকার জন্য আমার দুইটা <em>axis</em> বা অক্ষ ব্যবহার করতে হয়েছে। আপাতত এটাই ভেক্টর এর পরিচিতি।</p> <h2 id="যোগ">যোগ</h2> <p>উপরের (a,b) ভেক্টর এর যোগ করা যায় সহজেই। এদের সদস্যগুলোকে একটা আরেকটার সাথে যোগ করলেই হয়। মানে, (u = a+b=(3,3))। কিন্তু বিষয় হল এই যোগ কি বুঝায়। দুইটা ভেক্টর এর যোগ হচ্ছে অনেকটা এরকম-</p> <blockquote> <p>একটা ভেক্টর এর শুরুর বিন্দু কে অন্য ভেক্টর এর শেষে নিয়ে যাওয়া।</p> </blockquote> <p>মানে আগে আমরা ((0,0)) কে ভেক্টর এর শুরুর বিন্দু হিসেবে ধরে নিতাম, কিন্তু যোগ করার পর নতুন যে ভেক্টর পাওয়া যায় তা হচ্ছে আগের একটা ভেক্টর কে আরেকটা ভেক্টরের মাথায় নিয়ে যাওয়া। সরানোর পর ভেক্টরটা যে বিন্দুতে পৌঁছাবে, ওইটা হল আমাদের নতুন ভেক্টর, (u)। ছবি ১.২ লক্ষ্য করি।</p> <p><img src="../assets/images/linear-algebra-ch01/2.png" alt="image2"/> <em>ছবি ১.২ ভেক্টর যোগ</em></p> <p>এখানে দুইটির যোগফল হল (u=(3,3))। কিন্তু ডানপাশে ডট ডট আরেকটা ভেক্টর আছে, ওইটা আসলে (a) এবং দেখা যাচ্ছে যে, (a) এর শুরু যদি (b) এর শেষ থেকে হয় তাহলে (a) উভয়ের যোগফলে গিয়ে শেষ হয়। এখানে (b) কে সরিয়ে (a) এর উপর নিলেও একই উত্তর আসবে।</p> <h2 id="গুন">গুন</h2> <p>কোন সংখ্যা দাড়া ভেক্টরকে গুন করলে ওই ভেক্টর শুধু লম্বায় বড় বা ছোট হয়। যেমন, (u = 3a = (3,6))।</p> <p><img src="../assets/images/linear-algebra-ch01/5.png" alt="3a"/></p> <h2 id="নোটেশন">নোটেশন</h2> <p>এতক্ষণ ভেক্টরকে দুইটি ব্র্যাকেটের মধ্যে রাখলেও যেকোনো ভেক্টর, (v) এর আসল রূপ হল এরকম, ) v = \begin{bmatrix} v_1 <br/> v_2 \end{bmatrix} ) শুধুমাত্র লেখকদের সুবিধার জন্য বেশিরভাগ বইয়ে (v=(v_1,v_2)) লেখা হয়।</p>]]></content><author><name></name></author><category term="math"/><category term="theory"/><summary type="html"><![CDATA[explaining permutation and combination with examples.]]></summary></entry><entry><title type="html">Idea and Implementation / Boosting</title><link href="https://riyadhrazzaq.github.io/blog/2020/boosting/" rel="alternate" type="text/html" title="Idea and Implementation / Boosting"/><published>2020-07-27T00:00:00+00:00</published><updated>2020-07-27T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/boosting</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/boosting/"><![CDATA[<h1 id="boosting">Boosting</h1> <p>Boosting is also a general approach that can be applied to many statistical methods. Unlike bagging, boosting use the same dataset. Boosting works on top of weak learners as in very simple algorithms or simple version of a powerful algorithm. For example, a decision tree with 1 split allowed is a weak learner. Output of that tree will be closer to random guessing. Boosting trains the same weak learner multiple times and after each training iterations, updates the dataset.</p> <h2 id="adaboost">AdaBoost</h2> <p>It is a binary classifier where (Y \in {-1, +1}). The algorithm begins with setting up weights (D_i = \frac{1}{N}) for each observation (X_i) where (i=1…N). Suppose we train (T) models, then output from the (t_{th}) model will be (G_t(x_i)). Before the training of (t_{th}) model, we update (X) as (X_i = X_i \times D_i). As we can see, it is element wise operation. This considers as “punishing” the training samples which were misclassified in the previous model. Now that we have punished and trained, we will calculate the error, (err_t)</p> <p>[ \frac{ \sum_{i=1}^{N} D_i \times I(y_i \ne G_t(x_i)) }{\sum_{i=1}^{N} D_i} ]</p> <p>(I(condition)) returns 1 or 0 if the condition is true or false respectively. Now we will have multiple models at our hand and when finally combining their results, we should give more priorities to the more accurate models. This is achieved by calculating, (\alpha_t = \log\left( \frac{1-err_t}{err_t} \right)). Also, we haven’t updated the weights (D_i). They should be updated based on their performance on (t_{th}) tree.</p> <p>[ D_i = D_i \times \exp(\alpha_t \times I(y_i \ne G_t(X_i))) ]</p> <p>using (I(y_i \ne G_t(X_i))) makes sure that we don’t update samples that have been correctly classified. Finally after training of (T) models, we combine their result using majority vote.</p> <p>[ G(x) = sign\left(\sum_{t=1}^{T} \alpha_t \times G_t(x)\right) ]</p> <p>(sign(x)) returns -1 if (x) is negative, 1 if (x) is positive, and 0 if (x) is zero.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdaBoost</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span> <span class="o">=</span> <span class="p">[]</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">nSamples</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nSamples</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">estimator_weights_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">estimator_errors_</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">error_indices</span> <span class="o">=</span> <span class="n">y</span> <span class="o">!=</span> <span class="n">predictions</span>
            <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">[</span><span class="n">error_indices</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">)</span>
            
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">err</span><span class="p">)</span> <span class="o">/</span> <span class="n">err</span><span class="p">)</span>
            
            <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">[</span><span class="n">error_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">D</span><span class="p">[</span><span class="n">error_indices</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span>
<span class="c1">#             storing meta information
</span>            <span class="n">self</span><span class="p">.</span><span class="n">estimator_weights_</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">estimator_errors_</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nSamples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">Y</span><span class="p">[:,</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">modelAlpha</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
        
</code></pre></div></div> <h3 id="trying-out-the-model">Trying out the model</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">1</span><span class="p">,.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,.</span><span class="mi">08</span><span class="p">,.</span><span class="mi">12</span><span class="p">,.</span><span class="mi">33</span><span class="p">,.</span><span class="mi">55</span><span class="p">,.</span><span class="mi">66</span><span class="p">,.</span><span class="mi">77</span><span class="p">,.</span><span class="mi">88</span><span class="p">,.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">5</span><span class="p">,.</span><span class="mi">6</span><span class="p">,.</span><span class="mi">25</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">5</span><span class="p">,.</span><span class="mi">7</span><span class="p">,.</span><span class="mi">6</span><span class="p">])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">65</span><span class="p">,.</span><span class="mi">7</span><span class="p">,.</span><span class="mi">6</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">1</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">66</span><span class="p">,.</span><span class="mi">77</span><span class="p">,.</span><span class="mi">65</span><span class="p">,.</span><span class="mi">68</span><span class="p">,.</span><span class="mi">55</span><span class="p">,.</span><span class="mi">44</span><span class="p">,.</span><span class="mi">1</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">4</span><span class="p">,.</span><span class="mi">3</span><span class="p">,.</span><span class="mi">15</span><span class="p">,.</span><span class="mi">15</span><span class="p">,.</span><span class="mi">5</span><span class="p">,.</span><span class="mi">55</span><span class="p">,.</span><span class="mi">2</span><span class="p">,.</span><span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)).</span><span class="n">T</span>

<span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">==-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div> <p>sklearn’s</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">boost</span> <span class="o">=</span> <span class="nc">AdaBoostClassifier</span><span class="p">(</span> <span class="n">base_estimator</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> 
                            <span class="n">algorithm</span> <span class="o">=</span> <span class="sh">'</span><span class="s">SAMME</span><span class="sh">'</span><span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">boost</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">boost</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.8695652173913043
</code></pre></div></div> <p>ours</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ab</span> <span class="o">=</span> <span class="nc">AdaBoost</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ab</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">ab</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.8695652173913043
</code></pre></div></div> <p>nice!</p> <h1 id="references">References</h1> <ol> <li>Freund, Yoav, and Robert E. Schapire. “A desicion-theoretic generalization of on-line learning and an application to boosting.” European conference on computational learning theory. Springer, Berlin, Heidelberg, 1995.</li> <li>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media, 2009.</li> <li>James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li> </ol>]]></content><author><name></name></author><category term="mahcine-learning"/><summary type="html"><![CDATA[Boosting Boosting is also a general approach that can be applied to many statistical methods. Unlike bagging, boosting use the same dataset. Boosting works on top of weak learners as in very simple algorithms or simple version of a powerful algorithm. For example, a decision tree with 1 split allowed is a weak learner. Output of that tree will be closer to random guessing. Boosting trains the same weak learner multiple times and after each training iterations, updates the dataset. AdaBoost It is a binary classifier where (Y \in {-1, +1}). The algorithm begins with setting up weights (D_i = \frac{1}{N}) for each observation (X_i) where (i=1…N). Suppose we train (T) models, then output from the (t_{th}) model will be (G_t(x_i)). Before the training of (t_{th}) model, we update (X) as (X_i = X_i \times D_i). As we can see, it is element wise operation. This considers as “punishing” the training samples which were misclassified in the previous model. Now that we have punished and trained, we will calculate the error, (err_t)]]></summary></entry><entry><title type="html">Idea and Implementation / Random Forest</title><link href="https://riyadhrazzaq.github.io/blog/2020/random-forest/" rel="alternate" type="text/html" title="Idea and Implementation / Random Forest"/><published>2020-07-26T00:00:00+00:00</published><updated>2020-07-26T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/random-forest</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/random-forest/"><![CDATA[<h1 id="random-forest">Random Forest</h1> <p>RF is similar to bagging with one difference. While training each of \(B\) datasets, bagging considers all the features \(p_i\), RF does not. This is a way to decorrelate individual models. RF is mostly use with decision tree and the idea behind it seems clear and intuitive in that case.</p> <p>In normal situation, one tree is grown with all features. And in these features, some might be strong predictors. Therefore, while growing multiple trees, most of them will use the strong features in their top split. Hence, the predictions will be highly correlated and our goal to reduce variance will be slightly derailed.</p> <p>A key property in RF is the choice of \(m\), the number of features to consider each split. If we use this feature subsets, then on average, \((p-m)/p\) splits will not even consider the strong predictors, and others will. This will decorrelate the trees, and the average of the predictions will be less variable.</p> <h2 id="implementation">Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RF</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Implements RF from scratch with scikit learn</span><span class="sh">'</span><span class="s">s DecisionTreeRegressor.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        params
        ------
        B: int.
            Number of trees in the forest.
        
        m: int.
            Number of features to consider in each tree.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nc">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)</span> <span class="c1"># max_features is sklearn's 'm' variable
</span>            <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">nSamples</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nSamples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">Y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span>
        <span class="k">return</span> <span class="n">y</span>        
</code></pre></div></div> <h2 id="trying-out-the-models">Trying out the models</h2> <p>sklearn’s</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">sk</span> <span class="o">=</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">sk</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">sk</span><span class="p">.</span><span class="nf">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([2.27887587])
</code></pre></div></div> <p>ours</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rfr</span> <span class="o">=</span> <span class="nc">RF</span><span class="p">(</span><span class="n">B</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">rfr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">rfr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([3.20605521])
</code></pre></div></div> <h1 id="references">References</h1> <ol> <li>Freund, Yoav, and Robert E. Schapire. “A desicion-theoretic generalization of on-line learning and an application to boosting.” European conference on computational learning theory. Springer, Berlin, Heidelberg, 1995.</li> <li>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media, 2009.</li> <li>James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Random Forest]]></summary></entry><entry><title type="html">Idea and Implementation / Bagging</title><link href="https://riyadhrazzaq.github.io/blog/2020/bagging/" rel="alternate" type="text/html" title="Idea and Implementation / Bagging"/><published>2020-07-25T00:00:00+00:00</published><updated>2020-07-25T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/bagging</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/bagging/"><![CDATA[<p>Bagging, random forest, and boosting are statistical approach to further enhance already available algorithms. They all seem to deal with multiple training of a models on the samples from one dataset. Let’s look at the idea behind these.</p> <h1 id="bagging">Bagging</h1> <p>Consider a decision tree. When I split a dataset, and fit two distinct decision tree on those two halves, it will give different outputs. Meaning, our models will have high variance. Bagging or Bootstrap Aggragation comes from this problem. It is a general purpose approach for reducing the variance of models.</p> <p>Theoretically, averaging samples reduces the variance. So, to solve the problem above, we can just train multiple models, get their predictions, average them, and voila! This is what bagging is. But how can we get multiple dataset though? We use <a href="https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29">bootstrap</a> for that. Bootstrap will generate B different training datasets from one single dataset. We will train on \(b_{th}\) dataset to get \(f_b(x)\). When producing prediction, we will do average of all the \(f_b(x)\). For a single sample \(x\), the equation stands,</p> \[\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^{B} f_b(x)\] <h2 id="implementation-in-python">Implementation in Python</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span><span class="p">,</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="n">importlib</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Bagging</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">modelName</span><span class="o">=</span><span class="sh">'</span><span class="s">sklearn.linear_model.LinearRegression</span><span class="sh">'</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        params
        ------
        B: int. 
            B separate datasets will be generated.
        
        modelName: str. 
            A sklearn Estimator for regression. Module name and model name must be separated by dot(.). Default: </span><span class="sh">"</span><span class="s">sklearn.linear_model.LinearRegressor</span><span class="sh">"</span><span class="s">
        
        returns
        -------
        
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelName</span> <span class="o">=</span> <span class="n">modelName</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Fits B model to B datasets.
        
        params
        ------
        X, y: typical input ndarray.
        </span><span class="sh">"""</span>
        <span class="n">nSamples</span><span class="p">,</span> <span class="n">nFeats</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">nSamples</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">importlib</span><span class="p">.</span><span class="nf">import_module</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">modelName</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">modelName</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">estimator</span> <span class="o">=</span> <span class="nf">eval</span><span class="p">(</span><span class="sh">'</span><span class="s">self.module.</span><span class="sh">'</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">modelName</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="sh">"</span><span class="s">()</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">],</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Averages outputs from B model to predict each observations.
        
        params
        ------
        X: typical input ndarray.
        </span><span class="sh">"""</span>
        <span class="n">nSamples</span><span class="p">,</span> <span class="n">nFeats</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nSamples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">))</span> <span class="c1"># output for each model is a column vector. output for each sample is row vector
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">modelContainer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">Y</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">B</span>
        
        <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div> <h2 id="trying-out-the-model">Trying out the model</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                       <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_targets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((100, 4), (100,))
</code></pre></div></div> <p>sklearn’s model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">regr</span> <span class="o">=</span> <span class="nc">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="nc">LinearRegression</span><span class="p">(),</span>
                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">regr</span><span class="p">.</span><span class="nf">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1.54321e-15])
</code></pre></div></div> <p>Our Model</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bag</span> <span class="o">=</span> <span class="nc">Bagging</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">bag</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">bag</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([6.99440506e-16])
</code></pre></div></div> <p>Nice!</p> <h1 id="references">References</h1> <ol> <li>Freund, Yoav, and Robert E. Schapire. “A desicion-theoretic generalization of on-line learning and an application to boosting.” European conference on computational learning theory. Springer, Berlin, Heidelberg, 1995.</li> <li>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media, 2009.</li> <li>James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Bagging, random forest, and boosting are statistical approach to further enhance already available algorithms. They all seem to deal with multiple training of a models on the samples from one dataset. Let’s look at the idea behind these.]]></summary></entry><entry><title type="html">Idea and Implementation / Multinomial Naive Bayes</title><link href="https://riyadhrazzaq.github.io/blog/2020/multi-naive-bayes/" rel="alternate" type="text/html" title="Idea and Implementation / Multinomial Naive Bayes"/><published>2020-07-05T00:00:00+00:00</published><updated>2020-07-05T00:00:00+00:00</updated><id>https://riyadhrazzaq.github.io/blog/2020/multi-naive-bayes</id><content type="html" xml:base="https://riyadhrazzaq.github.io/blog/2020/multi-naive-bayes/"><![CDATA[<p>Multinomial naive bayes is the naive Bayes algorithm for multinomially distributed data. For a brief and intuitive explanation of Bayes theorem, read this kernel of mine: <a href="https://www.kaggle.com/riyadhrazzaq/gaussian-naive-bayes-classifier">Gaussian Naive Bayes Classifier from Scratch</a>. Everything is similar to Gaussian NB except the \(P(x_i \mid y)\). The new equation is, \(P(x_i \mid y) = \frac{N_{yi} + \alpha}{N_y + \alpha n} \label{eq1}\tag{1}\) Here,</p> <ul> <li>\(\alpha\) is the smoothing parameter,</li> <li>\(N_{yi}\) is the count of feature \(x_i\) in class y.</li> <li>\(N_y\) is the total count of all features in class y</li> <li>\(n\) is the total number of features</li> </ul> <h1 id="multinomial-naive-bayes">Multinomial Naive Bayes</h1> <p>You can look up in detail about multinomial distribution and you should. I will only put a short description of how a multinomial naive bayes classifier considers data.</p> <h2 id="multinomial-data">Multinomial Data</h2> <table> <thead> <tr> <th>\(X_1\)</th> <th>\(X_2\)</th> <th>\(X_3\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0</td> <td>4</td> </tr> <tr> <td>4</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <p>In the table above containing 2 sample of 3 features, we observe that feature \(X_1\) has values 1 and 4, and so on. That is the common view of the data. And when other a general model accepts this data, it considers each number as value. For example, \(X_{1,2}=3\). But in case of reading a multinomial data, \(X_{1,2}\) says how many of feature \(X_{2}\) is in sample 1. Meaning \(X_{1,2}\) is not value of the feature, instead it is the count of the feature. Let’s consider a text corpus. Each sentence is made up of different words \(w_i\) and each of those \(w_i\) belongs to the vocabulary, \(V\). If \(V\) contains 8 words, \(w_1,w_2,...,w_8\) and if a sentence is: w1 w2 w2 w6 w3 w2 w8, the representation of that sentence will be-</p> <table> <thead> <tr> <th>\(w_1\)</th> <th>\(w_2\)</th> <th>\(w_3\)</th> <th>\(w_4\)</th> <th>\(w_5\)</th> <th>\(w_6\)</th> <th>\(w_7\)</th> <th>\(w_8\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>3</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>1</td> </tr> </tbody> </table> <p>After inserting some other random sentences, the dataset is-</p> <table> <thead> <tr> <th>\(w_1\)</th> <th>\(w_2\)</th> <th>\(w_3\)</th> <th>\(w_4\)</th> <th>\(w_5\)</th> <th>\(w_6\)</th> <th>\(w_7\)</th> <th>\(w_8\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>3</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>1</td> </tr> <tr> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> <td>3</td> </tr> <tr> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>2</td> <td>1</td> <td>2</td> </tr> </tbody> </table> <p>By the way, I haven’t put them in a class. Randomly taking, \(y\) = [1,0,1]. Now, comparing with the equation of above,</p> <ul> <li>\(N_{yi}\) is the count of feature \(w_i\) in each unique class of y. For example, for \(y=1\), \(N_{y,1}=1, N_{y,6}=3\)</li> <li>\(N_y\) is the total count of all features in each unique class of y. For example, for \(y=1\), \(N_y=12\)</li> <li>\(n=8\) is the total number of features</li> <li>\(\alpha\) is known as smoothing parameter. It is needed for zero probability problem which is explained in resource [1]</li> </ul> <p>To calculate likelyhoods for a test sentence, all we need is \(P(w_i \mid y)\) which will be used to calculate \(P(X \mid y)\) from training data. But \(P(w_i \mid y)\) is the probability of feature \(w_i\) appearing under class y once. If our test sentence has any feature \(w_i\) n times, we will need to include \(P(w_i \mid y)\) in \(P(X \mid y)\) n times too. So, final equation for \(P(X_i \mid y)\) will be-</p> \[P(X_i \mid y) = P(w_1 \mid y)^{X_{i,1}} \times P(w_2 \mid y)^{X_{i,2}} \times ... \times P(w_n \mid y)^{X_{i,n}}\] <p>Resources:</p> <ol> <li>https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn07-notes-nup.pdf</li> <li>https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes</li> </ol> <h3 id="some-libraries-and-test-data">Some libraries and test data</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span> 
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># test data
</span><span class="n">tmpX1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="sh">"</span><span class="s">2   0   0   0   1   2   3   1  0   0   1   0   2   1   0   0  0   1   0   1   0   2   1   0  1   0   0   2   0   1   0   1  2   0   0   0   1   0   1   3  0   0   1   2   0   0   2   1</span><span class="sh">"</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">  </span><span class="sh">"</span><span class="p">)])</span>
<span class="n">tmpX2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="sh">"</span><span class="s">0   1   1   0   0   0   1   0  1   2   0   1   0   0   1   1  0   1   1   0   0   2   0   0  0   0   0   0   0   0   0   0  0   0   1   0   1   0   1   0</span><span class="sh">"</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">  </span><span class="sh">"</span><span class="p">)])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">tmpX1</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">tmpX2</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">X and Y shapes</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X and Y shapes
 (11, 8) (11,)
</code></pre></div></div> <h1 id="class-multinb">Class MultiNB</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiNB</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    
    <span class="k">def</span> <span class="nf">_prior</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># CHECKED
</span>        <span class="sh">"""</span><span class="s">
        Calculates prior for each unique class in y. P(y)
        </span><span class="sh">"""</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_samples</span>
        <span class="k">return</span> <span class="n">P</span>
            
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="c1"># CHECKED, matches with sklearn
</span>        <span class="sh">"""</span><span class="s">
        Calculates the following things- 
            class_priors_ is list of priors for each y.
            N_yi: 2D array. Contains for each class in y, the number of time each feature i appears under y.
            N_y: 1D array. Contains for each class in y, the number of all features appear under y.
            
        params
        ------
        X: 2D array. shape(n_samples, n_features)
            Multinomial data
        y: 1D array. shape(n_samples,). Labels must be encoded to integers.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">class_priors_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_prior</span><span class="p">()</span>
        
        <span class="c1"># distinct values in each features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">uniques</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">):</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">uniques</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span> <span class="n">tmp</span> <span class="p">)</span>
            
        <span class="n">self</span><span class="p">.</span><span class="n">N_yi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">))</span> <span class="c1"># feature count
</span>        <span class="n">self</span><span class="p">.</span><span class="n">N_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span> <span class="c1"># total count 
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">:</span> <span class="c1"># x axis
</span>            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argwhere</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
            <span class="n">columnwise_sum</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">):</span> <span class="c1"># y axis
</span>                <span class="n">columnwise_sum</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span><span class="n">j</span><span class="p">]))</span>
                
            <span class="n">self</span><span class="p">.</span><span class="n">N_yi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">columnwise_sum</span> <span class="c1"># 2d
</span>            <span class="n">self</span><span class="p">.</span><span class="n">N_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">columnwise_sum</span><span class="p">)</span> <span class="c1"># 1d
</span>            
    <span class="k">def</span> <span class="nf">_theta</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Calculates theta_yi. aka P(xi | y) using eqn(1) in the notebook.
        
        params
        ------
        x_i: int. 
            feature x_i
            
        i: int.
            feature index. 
            
        h: int or string.
            a class in y
        
        returns
        -------
        theta_yi: P(xi | y)
        </span><span class="sh">"""</span>
        
        <span class="n">Nyi</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">N_yi</span><span class="p">[</span><span class="n">h</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
        <span class="n">Ny</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">N_y</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
        
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">Nyi</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">Ny</span> <span class="o">+</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_features</span><span class="p">)</span>
        
        <span class="nf">return  </span><span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">)</span><span class="o">**</span><span class="n">x_i</span>
    
    <span class="k">def</span> <span class="nf">_likelyhood</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Calculates P(E|H) = P(E1|H) * P(E2|H) .. * P(En|H).
        
        params
        ------
        x: array. shape(n_features,)
            a row of data.
        h: int. 
            a class in y
        </span><span class="sh">"""</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">tmp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_theta</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span><span class="n">h</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">self</span><span class="p">.</span><span class="n">predict_proba</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">samples</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">joint_likelyhood</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">))</span>
            
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">):</span>
                <span class="n">joint_likelyhood</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">class_priors_</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">_likelyhood</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">h</span><span class="p">)</span> <span class="c1"># P(y) P(X|y) 
</span>                
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">joint_likelyhood</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_classes_</span><span class="p">):</span>
                <span class="n">numerator</span> <span class="o">=</span> <span class="n">joint_likelyhood</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
                <span class="n">self</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">)</span>
            
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Sklearn Sanity Check
    </span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">,</span><span class="sh">'</span><span class="s">Sklearn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">sk_y</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Count </span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">feature_count_</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Class Log Prior </span><span class="sh">"</span><span class="p">,</span><span class="n">clf</span><span class="p">.</span><span class="n">class_log_prior_</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy </span><span class="sh">'</span><span class="p">,</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">sk_y</span><span class="p">),</span><span class="n">sk_y</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">,</span><span class="sh">'</span><span class="s">Custom</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">nb</span> <span class="o">=</span> <span class="nc">MultiNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">nb</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">nb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">me_score</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Count</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">nb</span><span class="p">.</span><span class="n">N_yi</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Class Log Prior </span><span class="sh">"</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">class_priors_</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy </span><span class="sh">'</span><span class="p">,</span><span class="n">me_score</span><span class="p">,</span><span class="n">yhat</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">)</span> <span class="c1"># my predict proba is only for last test set
</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">pipeline</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-------------------- Sklearn --------------------
Feature Count 
 [[5. 1. 2. 5. 4. 6. 7. 6.]
 [1. 4. 3. 1. 1. 2. 3. 1.]]
Class Log Prior  [-0.6061358  -0.78845736]
Accuracy  0.8181818181818182 [0 0 0 0 0 0 1 1 1 0 0]
[[0.74940942 0.25059058]
 [0.52879735 0.47120265]
 [0.53711475 0.46288525]
 [0.69613326 0.30386674]
 [0.75239818 0.24760182]
 [0.62207341 0.37792659]
 [0.39213534 0.60786466]
 [0.45705923 0.54294077]
 [0.42055705 0.57944295]
 [0.54545455 0.45454545]
 [0.51099295 0.48900705]]
-------------------- Custom --------------------
Feature Count
 [[5. 1. 2. 5. 4. 6. 7. 6.]
 [1. 4. 3. 1. 1. 2. 3. 1.]]
Class Log Prior  [-0.6061358  -0.78845736]
Accuracy  0.8181818181818182 [0 0 0 0 0 0 1 1 1 0 0]
[[0.74940942 0.25059058]
 [0.52879735 0.47120265]
 [0.53711475 0.46288525]
 [0.69613326 0.30386674]
 [0.75239818 0.24760182]
 [0.62207341 0.37792659]
 [0.39213534 0.60786466]
 [0.45705923 0.54294077]
 [0.42055705 0.57944295]
 [0.54545455 0.45454545]
 [0.51099295 0.48900705]]
</code></pre></div></div> <h1 id="spam-classification">Spam Classification</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">import</span> <span class="n">string</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">../data/spam_uci.csv</span><span class="sh">"</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">iso8859_14</span><span class="sh">'</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <h2 id="simple-preprocessing">Simple Preprocessing</h2> <p>to cleanup punctuations and stopwords</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">clean_util</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">punc_rmv</span> <span class="o">=</span> <span class="p">[</span><span class="n">char</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">]</span>
    <span class="n">punc_rmv</span> <span class="o">=</span> <span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">punc_rmv</span><span class="p">)</span>
    <span class="n">stopword_rmv</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">punc_rmv</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">w</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)]</span>
    
    <span class="k">return</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">stopword_rmv</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="n">clean_util</span><span class="p">)</span>
</code></pre></div></div> <h1 id="vectorizing">Vectorizing</h1> <p>Conforming the texts to the multinomial format we have discussed in the beginning. Also, classes in y must be converted to integers as I forgot to account for strings in my implementation and too lazy to update •͡˘㇁•͡˘</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cv</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cv</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]).</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">lb</span> <span class="o">=</span> <span class="nc">LabelBinarizer</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lb</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]).</span><span class="nf">ravel</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(5572, 9381) (5572,)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train Test Split
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(4179, 9381) (1393, 9381) (4179,) (1393,)
</code></pre></div></div> <p>sklearn’s <code class="language-plaintext highlighter-rouge">MultinomialNB</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sk</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">sk</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9755922469490309
</code></pre></div></div> <p>our <code class="language-plaintext highlighter-rouge">MultiNB</code> (⌐■_■)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">me</span> <span class="o">=</span> <span class="nc">MultiNB</span><span class="p">()</span>
<span class="n">me</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">me</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">yhat</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9755922469490309
CPU times: user 1min 5s, sys: 0 ns, total: 1min 5s
Wall time: 1min 5s
</code></pre></div></div> <p>It takes a lot of time but does not matter as it is a reference implementation only ヽ(｀Д´)ﾉ</p> <p><strong>I wrote the scratch implementation for my learning, if you see any error or typo, please let me know.</strong></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Multinomial naive bayes is the naive Bayes algorithm for multinomially distributed data. For a brief and intuitive explanation of Bayes theorem, read this kernel of mine: Gaussian Naive Bayes Classifier from Scratch. Everything is similar to Gaussian NB except the \(P(x_i \mid y)\). The new equation is, \(P(x_i \mid y) = \frac{N_{yi} + \alpha}{N_y + \alpha n} \label{eq1}\tag{1}\) Here, \(\alpha\) is the smoothing parameter, \(N_{yi}\) is the count of feature \(x_i\) in class y. \(N_y\) is the total count of all features in class y \(n\) is the total number of features]]></summary></entry></feed>